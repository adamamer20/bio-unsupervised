{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOsriY+oNQVXKpMyJTpRSPt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isottongloria/PMLS_Bio-Learning/blob/main/scripts/multilayer_bio_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Unsupervised learning by competing hidden units**\n",
        "\n",
        " ### Author\n",
        "\n",
        " - Gloria Isotton\n",
        " - Master degree in Physics of data\n",
        "\n",
        "### Introduction\n",
        "\n",
        "In this notebook, we extend the approach presented in the paper to develop a more sophisticated unsupervised learning framework capable of training a **multi-layer network**. The goal is to leverage the unsupervised learning paradigm in a more complex network structure while incorporating **static excitatory and inhibitory synaptic connections** to improve the realism and performance of the model.\n",
        "\n",
        "The extension involves multiple layers of neurons, where each layer learns independently in an unsupervised manner, with connections between layers being modulated by excitatory and inhibitory synapses. This setup mimics the dynamics of biological neural networks more closely, where the balance between excitation and inhibition plays a crucial role in network stability, learning efficiency, and the emergence of meaningful representations.\n",
        "\n",
        "In the following sections, we will present the modifications made to the original framework, explain the design choices for synaptic regulation, and showcase the performance of the multi-layer model on several benchmark datasets. The code and the experiments demonstrate how unsupervised learning can be scaled to deeper architectures while preserving biological plausibility through synaptic balance."
      ],
      "metadata": {
        "id": "VwnS7783pNv8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing dependencies\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from PIL import Image\n",
        "from torch import nn,save,load\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split, TensorDataset\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import ExponentialLR\n",
        "from torch.optim.lr_scheduler import MultiStepLR\n",
        "import os\n"
      ],
      "metadata": {
        "id": "7jIoEBcMqPE9"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cuda Parameters\n",
        "use_cuda = torch.cuda.is_available()\n",
        "torch.cuda.empty_cache()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
      ],
      "metadata": {
        "id": "aPVUYk6KqiUu"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset loading\n",
        "Variable containing the dataset name `data_name`:\n",
        "- `1` : MNIST https://pytorch.org/vision/stable/generated/torchvision.datasets.MNIST.html?ref=hackernoon.com#torchvision.datasets.MNIST <br>\n",
        "- `2` : CIFAR10 https://pytorch.org/vision/stable/generated/torchvision.datasets.CIFAR10.html?ref=hackernoon.com#torchvision.datasets.CIFAR10<br>\n",
        "- `3` : FashionMNIST https://pytorch.org/vision/stable/generated/torchvision.datasets.FashionMNIST.html?ref=hackernoon.com#torchvision.datasets.FashionMNIST<br>"
      ],
      "metadata": {
        "id": "BHl0S9nFqUc2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_name=3\n",
        "\n",
        "####### MNIST dataset ########\n",
        "if data_name == 1:\n",
        "    transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                   transforms.Lambda(lambda x: x.view(-1))])\n",
        "    full_train_dataset = datasets.MNIST(root=\"data\", download=True, train=True, transform=transform)\n",
        "\n",
        "    train_size = 50000\n",
        "    val_size = 10000\n",
        "    train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=28, shuffle=True)\n",
        "    test_loader = DataLoader(val_dataset, batch_size=28, shuffle=False)\n",
        "\n",
        "    print(f\"Training set size: {len(train_loader.dataset)}\")\n",
        "    print(f\"Test set size: {len(test_loader.dataset)}\")\n",
        "\n",
        "\n",
        "####### CIFAR10 dataset ########\n",
        "if data_name == 2:\n",
        "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "                                   transforms.Lambda(lambda x: x.view(-1))])\n",
        "    full_train_dataset = datasets.CIFAR10(root=\"data\", download=True, train=True, transform=transform)\n",
        "\n",
        "    train_size = 40000\n",
        "    val_size = 10000\n",
        "    train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "    test_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    print(f\"Training set size: {len(train_loader.dataset)}\")\n",
        "    print(f\"Test set size: {len(test_loader.dataset)}\")\n",
        "\n",
        "\n",
        "\n",
        "####### FASHION MNIST dataset ########\n",
        "if data_name == 3:\n",
        "    transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,), (0.5,)),\n",
        "                                   transforms.Lambda(lambda x: x.view(-1))])\n",
        "\n",
        "    full_train_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True,download=True, transform=transform)\n",
        "    train_size = 50000\n",
        "    val_size = 10000\n",
        "    train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=28,shuffle=True)\n",
        "    test_loader = torch.utils.data.DataLoader(val_dataset, batch_size=28,shuffle=False)\n",
        "\n",
        "    print(f\"Training set size: {len(train_loader.dataset)}\")\n",
        "    print(f\"Test set size: {len(test_loader.dataset)}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "qgye8y5RqSUq",
        "outputId": "75cb26a6-e962-487c-ba74-8805124c727b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 50000\n",
            "Test set size: 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Single layer unsupervised training**"
      ],
      "metadata": {
        "id": "dLamDhDOALx_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def unsupervised_bio_learning(train_dataset, initial_synapses, n_hidden=28, n_epochs=500, batch_size=100,\n",
        "                              learning_rate=0.01, precision=0.1,\n",
        "                              anti_hebbian_learning_strength=0.3,\n",
        "                              lebesgue_norm=2, rank=5, skip=1):\n",
        "    \"\"\"\n",
        "    Unsupervised bio learning function.\n",
        "\n",
        "    Parameters:\n",
        "    - train_dataset: Input dataset (torch.Tensor), where each row is a training example.\n",
        "    - n_input: number of input neurons\n",
        "    - n_hidden: Number of hidden units (neurons).\n",
        "    - n_epochs: Number of epochs to train the model.\n",
        "    - batch_size: The size of the minibatch used to update the weights.\n",
        "    - learning_rate: Initial learning rate that decreases over epochs.\n",
        "    - precision: A threshold to normalize the gradient to avoid very small updates.\n",
        "    - anti_hebbian_learning_strength: Strength of anti-Hebbian learning (penalizing neurons with low activation).\n",
        "    - lebesgue_norm: Parameter for the Lebesgue norm used to weigh the contributions of the weights.\n",
        "    - rank: Number of hidden neurons that are penalized using anti-Hebbian learning.\n",
        "    - skip: Print the number of epochs every skip-times.\n",
        "    \"\"\"\n",
        "\n",
        "    # Flatten the input data and determine input size\n",
        "    input_data = torch.stack([data[0].flatten() for data in train_dataset]).to(device)\n",
        "    n_input = input_data.shape[1]  # Number of input neurons\n",
        "\n",
        "    # Initialize synapse weights\n",
        "    synapses = torch.rand((n_hidden, n_input), dtype=torch.float, device=device)\n",
        "\n",
        "    if initial_synapses is not None:\n",
        "        synapses = initial_synapses.to(device)\n",
        "\n",
        "    # Loop over epochs\n",
        "    for epoch in range(n_epochs):\n",
        "        if (epoch % skip == 0):\n",
        "            print('Epoch -->', epoch)\n",
        "\n",
        "        eps = learning_rate * (1 - epoch / n_epochs)  # Decaying learning rate\n",
        "\n",
        "        # Shuffle dataset once for the entire epoch\n",
        "        shuffled_epoch_data = input_data[torch.randperm(input_data.shape[0]), :]\n",
        "\n",
        "        # Loop through minibatches\n",
        "        for i in range(0, len(train_dataset), batch_size):\n",
        "            mini_batch = shuffled_epoch_data[i:i + batch_size, :].to(device)\n",
        "            mini_batch = mini_batch.transpose(0, 1)  # Transpose for correct shape (n_input, batch_size)\n",
        "\n",
        "            # --- Currents --- #\n",
        "            sign = torch.sign(synapses).to(device)  # Sign of weights\n",
        "            tot_input = torch.mm(sign*torch.abs(synapses).pow(lebesgue_norm-1), mini_batch) # Weight matrix raised to the power of (p-1)\n",
        "                                                                               # Compute the total input to the hidden layer\n",
        "            # --- Activation --- #\n",
        "            y = torch.argsort(tot_input, dim=0).to(device)  # Sort activations\n",
        "\n",
        "            # Initialize the Hebbian and anti-Hebbian activations matrix\n",
        "            yl = torch.zeros((n_hidden, batch_size)).to(device)\n",
        "\n",
        "            # Apply Hebbian learning (max activation)\n",
        "            yl[y[n_hidden - 1, :], torch.arange(batch_size, device=device)] = 1.0\n",
        "\n",
        "            # Apply anti-Hebbian learning (penalize lowest activations)\n",
        "            yl[y[n_hidden - rank, :], torch.arange(batch_size, device=device)] = -anti_hebbian_learning_strength\n",
        "\n",
        "            # Compute the contribution of the activations on the total input received\n",
        "            xx = torch.sum(torch.mul(yl, tot_input), 1)  # Sum over batch dimension\n",
        "\n",
        "            # --- Compute change of weights --- #\n",
        "            #ds = torch.mm(yl, mini_batch.transpose(0, 1)) - xx * synapses\n",
        "            ds = torch.matmul(yl, torch.transpose(mini_batch, 0, 1)) - torch.mul(xx.reshape(xx.shape[0],1).repeat(1, n_input), synapses)\n",
        "\n",
        "            # Normalize the gradient to prevent very large or very small updates\n",
        "            nc = torch.max(torch.abs(ds))\n",
        "            if nc < precision:\n",
        "                nc = precision\n",
        "\n",
        "            # Update the synapse weights\n",
        "            synapses += torch.mul(torch.div(ds, nc), eps)\n",
        "\n",
        "    return synapses\n"
      ],
      "metadata": {
        "id": "JTWU8yKXo6cj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "synapses = unsupervised_bio_learning(train_dataset,initial_synapses=None, n_hidden=2000, n_epochs=1000, batch_size=100,\n",
        "                              learning_rate=0.01, precision=0.1,\n",
        "                              anti_hebbian_learning_strength=0.3,\n",
        "                              lebesgue_norm=2, rank=2, skip=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "CvHDg4WO0bqN",
        "outputId": "b7b2aa79-5141-49e9-88de-e0585359f935"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch --> 0\n",
            "Epoch --> 1\n",
            "Epoch --> 2\n",
            "Epoch --> 3\n",
            "Epoch --> 4\n",
            "Epoch --> 5\n",
            "Epoch --> 6\n",
            "Epoch --> 7\n",
            "Epoch --> 8\n",
            "Epoch --> 9\n",
            "Epoch --> 10\n",
            "Epoch --> 11\n",
            "Epoch --> 12\n",
            "Epoch --> 13\n",
            "Epoch --> 14\n",
            "Epoch --> 15\n",
            "Epoch --> 16\n",
            "Epoch --> 17\n",
            "Epoch --> 18\n",
            "Epoch --> 19\n",
            "Epoch --> 20\n",
            "Epoch --> 21\n",
            "Epoch --> 22\n",
            "Epoch --> 23\n",
            "Epoch --> 24\n",
            "Epoch --> 25\n",
            "Epoch --> 26\n",
            "Epoch --> 27\n",
            "Epoch --> 28\n",
            "Epoch --> 29\n",
            "Epoch --> 30\n",
            "Epoch --> 31\n",
            "Epoch --> 32\n",
            "Epoch --> 33\n",
            "Epoch --> 34\n",
            "Epoch --> 35\n",
            "Epoch --> 36\n",
            "Epoch --> 37\n",
            "Epoch --> 38\n",
            "Epoch --> 39\n",
            "Epoch --> 40\n",
            "Epoch --> 41\n",
            "Epoch --> 42\n",
            "Epoch --> 43\n",
            "Epoch --> 44\n",
            "Epoch --> 45\n",
            "Epoch --> 46\n",
            "Epoch --> 47\n",
            "Epoch --> 48\n",
            "Epoch --> 49\n",
            "Epoch --> 50\n",
            "Epoch --> 51\n",
            "Epoch --> 52\n",
            "Epoch --> 53\n",
            "Epoch --> 54\n",
            "Epoch --> 55\n",
            "Epoch --> 56\n",
            "Epoch --> 57\n",
            "Epoch --> 58\n",
            "Epoch --> 59\n",
            "Epoch --> 60\n",
            "Epoch --> 61\n",
            "Epoch --> 62\n",
            "Epoch --> 63\n",
            "Epoch --> 64\n",
            "Epoch --> 65\n",
            "Epoch --> 66\n",
            "Epoch --> 67\n",
            "Epoch --> 68\n",
            "Epoch --> 69\n",
            "Epoch --> 70\n",
            "Epoch --> 71\n",
            "Epoch --> 72\n",
            "Epoch --> 73\n",
            "Epoch --> 74\n",
            "Epoch --> 75\n",
            "Epoch --> 76\n",
            "Epoch --> 77\n",
            "Epoch --> 78\n",
            "Epoch --> 79\n",
            "Epoch --> 80\n",
            "Epoch --> 81\n",
            "Epoch --> 82\n",
            "Epoch --> 83\n",
            "Epoch --> 84\n",
            "Epoch --> 85\n",
            "Epoch --> 86\n",
            "Epoch --> 87\n",
            "Epoch --> 88\n",
            "Epoch --> 89\n",
            "Epoch --> 90\n",
            "Epoch --> 91\n",
            "Epoch --> 92\n",
            "Epoch --> 93\n",
            "Epoch --> 94\n",
            "Epoch --> 95\n",
            "Epoch --> 96\n",
            "Epoch --> 97\n",
            "Epoch --> 98\n",
            "Epoch --> 99\n",
            "Epoch --> 100\n",
            "Epoch --> 101\n",
            "Epoch --> 102\n",
            "Epoch --> 103\n",
            "Epoch --> 104\n",
            "Epoch --> 105\n",
            "Epoch --> 106\n",
            "Epoch --> 107\n",
            "Epoch --> 108\n",
            "Epoch --> 109\n",
            "Epoch --> 110\n",
            "Epoch --> 111\n",
            "Epoch --> 112\n",
            "Epoch --> 113\n",
            "Epoch --> 114\n",
            "Epoch --> 115\n",
            "Epoch --> 116\n",
            "Epoch --> 117\n",
            "Epoch --> 118\n",
            "Epoch --> 119\n",
            "Epoch --> 120\n",
            "Epoch --> 121\n",
            "Epoch --> 122\n",
            "Epoch --> 123\n",
            "Epoch --> 124\n",
            "Epoch --> 125\n",
            "Epoch --> 126\n",
            "Epoch --> 127\n",
            "Epoch --> 128\n",
            "Epoch --> 129\n",
            "Epoch --> 130\n",
            "Epoch --> 131\n",
            "Epoch --> 132\n",
            "Epoch --> 133\n",
            "Epoch --> 134\n",
            "Epoch --> 135\n",
            "Epoch --> 136\n",
            "Epoch --> 137\n",
            "Epoch --> 138\n",
            "Epoch --> 139\n",
            "Epoch --> 140\n",
            "Epoch --> 141\n",
            "Epoch --> 142\n",
            "Epoch --> 143\n",
            "Epoch --> 144\n",
            "Epoch --> 145\n",
            "Epoch --> 146\n",
            "Epoch --> 147\n",
            "Epoch --> 148\n",
            "Epoch --> 149\n",
            "Epoch --> 150\n",
            "Epoch --> 151\n",
            "Epoch --> 152\n",
            "Epoch --> 153\n",
            "Epoch --> 154\n",
            "Epoch --> 155\n",
            "Epoch --> 156\n",
            "Epoch --> 157\n",
            "Epoch --> 158\n",
            "Epoch --> 159\n",
            "Epoch --> 160\n",
            "Epoch --> 161\n",
            "Epoch --> 162\n",
            "Epoch --> 163\n",
            "Epoch --> 164\n",
            "Epoch --> 165\n",
            "Epoch --> 166\n",
            "Epoch --> 167\n",
            "Epoch --> 168\n",
            "Epoch --> 169\n",
            "Epoch --> 170\n",
            "Epoch --> 171\n",
            "Epoch --> 172\n",
            "Epoch --> 173\n",
            "Epoch --> 174\n",
            "Epoch --> 175\n",
            "Epoch --> 176\n",
            "Epoch --> 177\n",
            "Epoch --> 178\n",
            "Epoch --> 179\n",
            "Epoch --> 180\n",
            "Epoch --> 181\n",
            "Epoch --> 182\n",
            "Epoch --> 183\n",
            "Epoch --> 184\n",
            "Epoch --> 185\n",
            "Epoch --> 186\n",
            "Epoch --> 187\n",
            "Epoch --> 188\n",
            "Epoch --> 189\n",
            "Epoch --> 190\n",
            "Epoch --> 191\n",
            "Epoch --> 192\n",
            "Epoch --> 193\n",
            "Epoch --> 194\n",
            "Epoch --> 195\n",
            "Epoch --> 196\n",
            "Epoch --> 197\n",
            "Epoch --> 198\n",
            "Epoch --> 199\n",
            "Epoch --> 200\n",
            "Epoch --> 201\n",
            "Epoch --> 202\n",
            "Epoch --> 203\n",
            "Epoch --> 204\n",
            "Epoch --> 205\n",
            "Epoch --> 206\n",
            "Epoch --> 207\n",
            "Epoch --> 208\n",
            "Epoch --> 209\n",
            "Epoch --> 210\n",
            "Epoch --> 211\n",
            "Epoch --> 212\n",
            "Epoch --> 213\n",
            "Epoch --> 214\n",
            "Epoch --> 215\n",
            "Epoch --> 216\n",
            "Epoch --> 217\n",
            "Epoch --> 218\n",
            "Epoch --> 219\n",
            "Epoch --> 220\n",
            "Epoch --> 221\n",
            "Epoch --> 222\n",
            "Epoch --> 223\n",
            "Epoch --> 224\n",
            "Epoch --> 225\n",
            "Epoch --> 226\n",
            "Epoch --> 227\n",
            "Epoch --> 228\n",
            "Epoch --> 229\n",
            "Epoch --> 230\n",
            "Epoch --> 231\n",
            "Epoch --> 232\n",
            "Epoch --> 233\n",
            "Epoch --> 234\n",
            "Epoch --> 235\n",
            "Epoch --> 236\n",
            "Epoch --> 237\n",
            "Epoch --> 238\n",
            "Epoch --> 239\n",
            "Epoch --> 240\n",
            "Epoch --> 241\n",
            "Epoch --> 242\n",
            "Epoch --> 243\n",
            "Epoch --> 244\n",
            "Epoch --> 245\n",
            "Epoch --> 246\n",
            "Epoch --> 247\n",
            "Epoch --> 248\n",
            "Epoch --> 249\n",
            "Epoch --> 250\n",
            "Epoch --> 251\n",
            "Epoch --> 252\n",
            "Epoch --> 253\n",
            "Epoch --> 254\n",
            "Epoch --> 255\n",
            "Epoch --> 256\n",
            "Epoch --> 257\n",
            "Epoch --> 258\n",
            "Epoch --> 259\n",
            "Epoch --> 260\n",
            "Epoch --> 261\n",
            "Epoch --> 262\n",
            "Epoch --> 263\n",
            "Epoch --> 264\n",
            "Epoch --> 265\n",
            "Epoch --> 266\n",
            "Epoch --> 267\n",
            "Epoch --> 268\n",
            "Epoch --> 269\n",
            "Epoch --> 270\n",
            "Epoch --> 271\n",
            "Epoch --> 272\n",
            "Epoch --> 273\n",
            "Epoch --> 274\n",
            "Epoch --> 275\n",
            "Epoch --> 276\n",
            "Epoch --> 277\n",
            "Epoch --> 278\n",
            "Epoch --> 279\n",
            "Epoch --> 280\n",
            "Epoch --> 281\n",
            "Epoch --> 282\n",
            "Epoch --> 283\n",
            "Epoch --> 284\n",
            "Epoch --> 285\n",
            "Epoch --> 286\n",
            "Epoch --> 287\n",
            "Epoch --> 288\n",
            "Epoch --> 289\n",
            "Epoch --> 290\n",
            "Epoch --> 291\n",
            "Epoch --> 292\n",
            "Epoch --> 293\n",
            "Epoch --> 294\n",
            "Epoch --> 295\n",
            "Epoch --> 296\n",
            "Epoch --> 297\n",
            "Epoch --> 298\n",
            "Epoch --> 299\n",
            "Epoch --> 300\n",
            "Epoch --> 301\n",
            "Epoch --> 302\n",
            "Epoch --> 303\n",
            "Epoch --> 304\n",
            "Epoch --> 305\n",
            "Epoch --> 306\n",
            "Epoch --> 307\n",
            "Epoch --> 308\n",
            "Epoch --> 309\n",
            "Epoch --> 310\n",
            "Epoch --> 311\n",
            "Epoch --> 312\n",
            "Epoch --> 313\n",
            "Epoch --> 314\n",
            "Epoch --> 315\n",
            "Epoch --> 316\n",
            "Epoch --> 317\n",
            "Epoch --> 318\n",
            "Epoch --> 319\n",
            "Epoch --> 320\n",
            "Epoch --> 321\n",
            "Epoch --> 322\n",
            "Epoch --> 323\n",
            "Epoch --> 324\n",
            "Epoch --> 325\n",
            "Epoch --> 326\n",
            "Epoch --> 327\n",
            "Epoch --> 328\n",
            "Epoch --> 329\n",
            "Epoch --> 330\n",
            "Epoch --> 331\n",
            "Epoch --> 332\n",
            "Epoch --> 333\n",
            "Epoch --> 334\n",
            "Epoch --> 335\n",
            "Epoch --> 336\n",
            "Epoch --> 337\n",
            "Epoch --> 338\n",
            "Epoch --> 339\n",
            "Epoch --> 340\n",
            "Epoch --> 341\n",
            "Epoch --> 342\n",
            "Epoch --> 343\n",
            "Epoch --> 344\n",
            "Epoch --> 345\n",
            "Epoch --> 346\n",
            "Epoch --> 347\n",
            "Epoch --> 348\n",
            "Epoch --> 349\n",
            "Epoch --> 350\n",
            "Epoch --> 351\n",
            "Epoch --> 352\n",
            "Epoch --> 353\n",
            "Epoch --> 354\n",
            "Epoch --> 355\n",
            "Epoch --> 356\n",
            "Epoch --> 357\n",
            "Epoch --> 358\n",
            "Epoch --> 359\n",
            "Epoch --> 360\n",
            "Epoch --> 361\n",
            "Epoch --> 362\n",
            "Epoch --> 363\n",
            "Epoch --> 364\n",
            "Epoch --> 365\n",
            "Epoch --> 366\n",
            "Epoch --> 367\n",
            "Epoch --> 368\n",
            "Epoch --> 369\n",
            "Epoch --> 370\n",
            "Epoch --> 371\n",
            "Epoch --> 372\n",
            "Epoch --> 373\n",
            "Epoch --> 374\n",
            "Epoch --> 375\n",
            "Epoch --> 376\n",
            "Epoch --> 377\n",
            "Epoch --> 378\n",
            "Epoch --> 379\n",
            "Epoch --> 380\n",
            "Epoch --> 381\n",
            "Epoch --> 382\n",
            "Epoch --> 383\n",
            "Epoch --> 384\n",
            "Epoch --> 385\n",
            "Epoch --> 386\n",
            "Epoch --> 387\n",
            "Epoch --> 388\n",
            "Epoch --> 389\n",
            "Epoch --> 390\n",
            "Epoch --> 391\n",
            "Epoch --> 392\n",
            "Epoch --> 393\n",
            "Epoch --> 394\n",
            "Epoch --> 395\n",
            "Epoch --> 396\n",
            "Epoch --> 397\n",
            "Epoch --> 398\n",
            "Epoch --> 399\n",
            "Epoch --> 400\n",
            "Epoch --> 401\n",
            "Epoch --> 402\n",
            "Epoch --> 403\n",
            "Epoch --> 404\n",
            "Epoch --> 405\n",
            "Epoch --> 406\n",
            "Epoch --> 407\n",
            "Epoch --> 408\n",
            "Epoch --> 409\n",
            "Epoch --> 410\n",
            "Epoch --> 411\n",
            "Epoch --> 412\n",
            "Epoch --> 413\n",
            "Epoch --> 414\n",
            "Epoch --> 415\n",
            "Epoch --> 416\n",
            "Epoch --> 417\n",
            "Epoch --> 418\n",
            "Epoch --> 419\n",
            "Epoch --> 420\n",
            "Epoch --> 421\n",
            "Epoch --> 422\n",
            "Epoch --> 423\n",
            "Epoch --> 424\n",
            "Epoch --> 425\n",
            "Epoch --> 426\n",
            "Epoch --> 427\n",
            "Epoch --> 428\n",
            "Epoch --> 429\n",
            "Epoch --> 430\n",
            "Epoch --> 431\n",
            "Epoch --> 432\n",
            "Epoch --> 433\n",
            "Epoch --> 434\n",
            "Epoch --> 435\n",
            "Epoch --> 436\n",
            "Epoch --> 437\n",
            "Epoch --> 438\n",
            "Epoch --> 439\n",
            "Epoch --> 440\n",
            "Epoch --> 441\n",
            "Epoch --> 442\n",
            "Epoch --> 443\n",
            "Epoch --> 444\n",
            "Epoch --> 445\n",
            "Epoch --> 446\n",
            "Epoch --> 447\n",
            "Epoch --> 448\n",
            "Epoch --> 449\n",
            "Epoch --> 450\n",
            "Epoch --> 451\n",
            "Epoch --> 452\n",
            "Epoch --> 453\n",
            "Epoch --> 454\n",
            "Epoch --> 455\n",
            "Epoch --> 456\n",
            "Epoch --> 457\n",
            "Epoch --> 458\n",
            "Epoch --> 459\n",
            "Epoch --> 460\n",
            "Epoch --> 461\n",
            "Epoch --> 462\n",
            "Epoch --> 463\n",
            "Epoch --> 464\n",
            "Epoch --> 465\n",
            "Epoch --> 466\n",
            "Epoch --> 467\n",
            "Epoch --> 468\n",
            "Epoch --> 469\n",
            "Epoch --> 470\n",
            "Epoch --> 471\n",
            "Epoch --> 472\n",
            "Epoch --> 473\n",
            "Epoch --> 474\n",
            "Epoch --> 475\n",
            "Epoch --> 476\n",
            "Epoch --> 477\n",
            "Epoch --> 478\n",
            "Epoch --> 479\n",
            "Epoch --> 480\n",
            "Epoch --> 481\n",
            "Epoch --> 482\n",
            "Epoch --> 483\n",
            "Epoch --> 484\n",
            "Epoch --> 485\n",
            "Epoch --> 486\n",
            "Epoch --> 487\n",
            "Epoch --> 488\n",
            "Epoch --> 489\n",
            "Epoch --> 490\n",
            "Epoch --> 491\n",
            "Epoch --> 492\n",
            "Epoch --> 493\n",
            "Epoch --> 494\n",
            "Epoch --> 495\n",
            "Epoch --> 496\n",
            "Epoch --> 497\n",
            "Epoch --> 498\n",
            "Epoch --> 499\n",
            "Epoch --> 500\n",
            "Epoch --> 501\n",
            "Epoch --> 502\n",
            "Epoch --> 503\n",
            "Epoch --> 504\n",
            "Epoch --> 505\n",
            "Epoch --> 506\n",
            "Epoch --> 507\n",
            "Epoch --> 508\n",
            "Epoch --> 509\n",
            "Epoch --> 510\n",
            "Epoch --> 511\n",
            "Epoch --> 512\n",
            "Epoch --> 513\n",
            "Epoch --> 514\n",
            "Epoch --> 515\n",
            "Epoch --> 516\n",
            "Epoch --> 517\n",
            "Epoch --> 518\n",
            "Epoch --> 519\n",
            "Epoch --> 520\n",
            "Epoch --> 521\n",
            "Epoch --> 522\n",
            "Epoch --> 523\n",
            "Epoch --> 524\n",
            "Epoch --> 525\n",
            "Epoch --> 526\n",
            "Epoch --> 527\n",
            "Epoch --> 528\n",
            "Epoch --> 529\n",
            "Epoch --> 530\n",
            "Epoch --> 531\n",
            "Epoch --> 532\n",
            "Epoch --> 533\n",
            "Epoch --> 534\n",
            "Epoch --> 535\n",
            "Epoch --> 536\n",
            "Epoch --> 537\n",
            "Epoch --> 538\n",
            "Epoch --> 539\n",
            "Epoch --> 540\n",
            "Epoch --> 541\n",
            "Epoch --> 542\n",
            "Epoch --> 543\n",
            "Epoch --> 544\n",
            "Epoch --> 545\n",
            "Epoch --> 546\n",
            "Epoch --> 547\n",
            "Epoch --> 548\n",
            "Epoch --> 549\n",
            "Epoch --> 550\n",
            "Epoch --> 551\n",
            "Epoch --> 552\n",
            "Epoch --> 553\n",
            "Epoch --> 554\n",
            "Epoch --> 555\n",
            "Epoch --> 556\n",
            "Epoch --> 557\n",
            "Epoch --> 558\n",
            "Epoch --> 559\n",
            "Epoch --> 560\n",
            "Epoch --> 561\n",
            "Epoch --> 562\n",
            "Epoch --> 563\n",
            "Epoch --> 564\n",
            "Epoch --> 565\n",
            "Epoch --> 566\n",
            "Epoch --> 567\n",
            "Epoch --> 568\n",
            "Epoch --> 569\n",
            "Epoch --> 570\n",
            "Epoch --> 571\n",
            "Epoch --> 572\n",
            "Epoch --> 573\n",
            "Epoch --> 574\n",
            "Epoch --> 575\n",
            "Epoch --> 576\n",
            "Epoch --> 577\n",
            "Epoch --> 578\n",
            "Epoch --> 579\n",
            "Epoch --> 580\n",
            "Epoch --> 581\n",
            "Epoch --> 582\n",
            "Epoch --> 583\n",
            "Epoch --> 584\n",
            "Epoch --> 585\n",
            "Epoch --> 586\n",
            "Epoch --> 587\n",
            "Epoch --> 588\n",
            "Epoch --> 589\n",
            "Epoch --> 590\n",
            "Epoch --> 591\n",
            "Epoch --> 592\n",
            "Epoch --> 593\n",
            "Epoch --> 594\n",
            "Epoch --> 595\n",
            "Epoch --> 596\n",
            "Epoch --> 597\n",
            "Epoch --> 598\n",
            "Epoch --> 599\n",
            "Epoch --> 600\n",
            "Epoch --> 601\n",
            "Epoch --> 602\n",
            "Epoch --> 603\n",
            "Epoch --> 604\n",
            "Epoch --> 605\n",
            "Epoch --> 606\n",
            "Epoch --> 607\n",
            "Epoch --> 608\n",
            "Epoch --> 609\n",
            "Epoch --> 610\n",
            "Epoch --> 611\n",
            "Epoch --> 612\n",
            "Epoch --> 613\n",
            "Epoch --> 614\n",
            "Epoch --> 615\n",
            "Epoch --> 616\n",
            "Epoch --> 617\n",
            "Epoch --> 618\n",
            "Epoch --> 619\n",
            "Epoch --> 620\n",
            "Epoch --> 621\n",
            "Epoch --> 622\n",
            "Epoch --> 623\n",
            "Epoch --> 624\n",
            "Epoch --> 625\n",
            "Epoch --> 626\n",
            "Epoch --> 627\n",
            "Epoch --> 628\n",
            "Epoch --> 629\n",
            "Epoch --> 630\n",
            "Epoch --> 631\n",
            "Epoch --> 632\n",
            "Epoch --> 633\n",
            "Epoch --> 634\n",
            "Epoch --> 635\n",
            "Epoch --> 636\n",
            "Epoch --> 637\n",
            "Epoch --> 638\n",
            "Epoch --> 639\n",
            "Epoch --> 640\n",
            "Epoch --> 641\n",
            "Epoch --> 642\n",
            "Epoch --> 643\n",
            "Epoch --> 644\n",
            "Epoch --> 645\n",
            "Epoch --> 646\n",
            "Epoch --> 647\n",
            "Epoch --> 648\n",
            "Epoch --> 649\n",
            "Epoch --> 650\n",
            "Epoch --> 651\n",
            "Epoch --> 652\n",
            "Epoch --> 653\n",
            "Epoch --> 654\n",
            "Epoch --> 655\n",
            "Epoch --> 656\n",
            "Epoch --> 657\n",
            "Epoch --> 658\n",
            "Epoch --> 659\n",
            "Epoch --> 660\n",
            "Epoch --> 661\n",
            "Epoch --> 662\n",
            "Epoch --> 663\n",
            "Epoch --> 664\n",
            "Epoch --> 665\n",
            "Epoch --> 666\n",
            "Epoch --> 667\n",
            "Epoch --> 668\n",
            "Epoch --> 669\n",
            "Epoch --> 670\n",
            "Epoch --> 671\n",
            "Epoch --> 672\n",
            "Epoch --> 673\n",
            "Epoch --> 674\n",
            "Epoch --> 675\n",
            "Epoch --> 676\n",
            "Epoch --> 677\n",
            "Epoch --> 678\n",
            "Epoch --> 679\n",
            "Epoch --> 680\n",
            "Epoch --> 681\n",
            "Epoch --> 682\n",
            "Epoch --> 683\n",
            "Epoch --> 684\n",
            "Epoch --> 685\n",
            "Epoch --> 686\n",
            "Epoch --> 687\n",
            "Epoch --> 688\n",
            "Epoch --> 689\n",
            "Epoch --> 690\n",
            "Epoch --> 691\n",
            "Epoch --> 692\n",
            "Epoch --> 693\n",
            "Epoch --> 694\n",
            "Epoch --> 695\n",
            "Epoch --> 696\n",
            "Epoch --> 697\n",
            "Epoch --> 698\n",
            "Epoch --> 699\n",
            "Epoch --> 700\n",
            "Epoch --> 701\n",
            "Epoch --> 702\n",
            "Epoch --> 703\n",
            "Epoch --> 704\n",
            "Epoch --> 705\n",
            "Epoch --> 706\n",
            "Epoch --> 707\n",
            "Epoch --> 708\n",
            "Epoch --> 709\n",
            "Epoch --> 710\n",
            "Epoch --> 711\n",
            "Epoch --> 712\n",
            "Epoch --> 713\n",
            "Epoch --> 714\n",
            "Epoch --> 715\n",
            "Epoch --> 716\n",
            "Epoch --> 717\n",
            "Epoch --> 718\n",
            "Epoch --> 719\n",
            "Epoch --> 720\n",
            "Epoch --> 721\n",
            "Epoch --> 722\n",
            "Epoch --> 723\n",
            "Epoch --> 724\n",
            "Epoch --> 725\n",
            "Epoch --> 726\n",
            "Epoch --> 727\n",
            "Epoch --> 728\n",
            "Epoch --> 729\n",
            "Epoch --> 730\n",
            "Epoch --> 731\n",
            "Epoch --> 732\n",
            "Epoch --> 733\n",
            "Epoch --> 734\n",
            "Epoch --> 735\n",
            "Epoch --> 736\n",
            "Epoch --> 737\n",
            "Epoch --> 738\n",
            "Epoch --> 739\n",
            "Epoch --> 740\n",
            "Epoch --> 741\n",
            "Epoch --> 742\n",
            "Epoch --> 743\n",
            "Epoch --> 744\n",
            "Epoch --> 745\n",
            "Epoch --> 746\n",
            "Epoch --> 747\n",
            "Epoch --> 748\n",
            "Epoch --> 749\n",
            "Epoch --> 750\n",
            "Epoch --> 751\n",
            "Epoch --> 752\n",
            "Epoch --> 753\n",
            "Epoch --> 754\n",
            "Epoch --> 755\n",
            "Epoch --> 756\n",
            "Epoch --> 757\n",
            "Epoch --> 758\n",
            "Epoch --> 759\n",
            "Epoch --> 760\n",
            "Epoch --> 761\n",
            "Epoch --> 762\n",
            "Epoch --> 763\n",
            "Epoch --> 764\n",
            "Epoch --> 765\n",
            "Epoch --> 766\n",
            "Epoch --> 767\n",
            "Epoch --> 768\n",
            "Epoch --> 769\n",
            "Epoch --> 770\n",
            "Epoch --> 771\n",
            "Epoch --> 772\n",
            "Epoch --> 773\n",
            "Epoch --> 774\n",
            "Epoch --> 775\n",
            "Epoch --> 776\n",
            "Epoch --> 777\n",
            "Epoch --> 778\n",
            "Epoch --> 779\n",
            "Epoch --> 780\n",
            "Epoch --> 781\n",
            "Epoch --> 782\n",
            "Epoch --> 783\n",
            "Epoch --> 784\n",
            "Epoch --> 785\n",
            "Epoch --> 786\n",
            "Epoch --> 787\n",
            "Epoch --> 788\n",
            "Epoch --> 789\n",
            "Epoch --> 790\n",
            "Epoch --> 791\n",
            "Epoch --> 792\n",
            "Epoch --> 793\n",
            "Epoch --> 794\n",
            "Epoch --> 795\n",
            "Epoch --> 796\n",
            "Epoch --> 797\n",
            "Epoch --> 798\n",
            "Epoch --> 799\n",
            "Epoch --> 800\n",
            "Epoch --> 801\n",
            "Epoch --> 802\n",
            "Epoch --> 803\n",
            "Epoch --> 804\n",
            "Epoch --> 805\n",
            "Epoch --> 806\n",
            "Epoch --> 807\n",
            "Epoch --> 808\n",
            "Epoch --> 809\n",
            "Epoch --> 810\n",
            "Epoch --> 811\n",
            "Epoch --> 812\n",
            "Epoch --> 813\n",
            "Epoch --> 814\n",
            "Epoch --> 815\n",
            "Epoch --> 816\n",
            "Epoch --> 817\n",
            "Epoch --> 818\n",
            "Epoch --> 819\n",
            "Epoch --> 820\n",
            "Epoch --> 821\n",
            "Epoch --> 822\n",
            "Epoch --> 823\n",
            "Epoch --> 824\n",
            "Epoch --> 825\n",
            "Epoch --> 826\n",
            "Epoch --> 827\n",
            "Epoch --> 828\n",
            "Epoch --> 829\n",
            "Epoch --> 830\n",
            "Epoch --> 831\n",
            "Epoch --> 832\n",
            "Epoch --> 833\n",
            "Epoch --> 834\n",
            "Epoch --> 835\n",
            "Epoch --> 836\n",
            "Epoch --> 837\n",
            "Epoch --> 838\n",
            "Epoch --> 839\n",
            "Epoch --> 840\n",
            "Epoch --> 841\n",
            "Epoch --> 842\n",
            "Epoch --> 843\n",
            "Epoch --> 844\n",
            "Epoch --> 845\n",
            "Epoch --> 846\n",
            "Epoch --> 847\n",
            "Epoch --> 848\n",
            "Epoch --> 849\n",
            "Epoch --> 850\n",
            "Epoch --> 851\n",
            "Epoch --> 852\n",
            "Epoch --> 853\n",
            "Epoch --> 854\n",
            "Epoch --> 855\n",
            "Epoch --> 856\n",
            "Epoch --> 857\n",
            "Epoch --> 858\n",
            "Epoch --> 859\n",
            "Epoch --> 860\n",
            "Epoch --> 861\n",
            "Epoch --> 862\n",
            "Epoch --> 863\n",
            "Epoch --> 864\n",
            "Epoch --> 865\n",
            "Epoch --> 866\n",
            "Epoch --> 867\n",
            "Epoch --> 868\n",
            "Epoch --> 869\n",
            "Epoch --> 870\n",
            "Epoch --> 871\n",
            "Epoch --> 872\n",
            "Epoch --> 873\n",
            "Epoch --> 874\n",
            "Epoch --> 875\n",
            "Epoch --> 876\n",
            "Epoch --> 877\n",
            "Epoch --> 878\n",
            "Epoch --> 879\n",
            "Epoch --> 880\n",
            "Epoch --> 881\n",
            "Epoch --> 882\n",
            "Epoch --> 883\n",
            "Epoch --> 884\n",
            "Epoch --> 885\n",
            "Epoch --> 886\n",
            "Epoch --> 887\n",
            "Epoch --> 888\n",
            "Epoch --> 889\n",
            "Epoch --> 890\n",
            "Epoch --> 891\n",
            "Epoch --> 892\n",
            "Epoch --> 893\n",
            "Epoch --> 894\n",
            "Epoch --> 895\n",
            "Epoch --> 896\n",
            "Epoch --> 897\n",
            "Epoch --> 898\n",
            "Epoch --> 899\n",
            "Epoch --> 900\n",
            "Epoch --> 901\n",
            "Epoch --> 902\n",
            "Epoch --> 903\n",
            "Epoch --> 904\n",
            "Epoch --> 905\n",
            "Epoch --> 906\n",
            "Epoch --> 907\n",
            "Epoch --> 908\n",
            "Epoch --> 909\n",
            "Epoch --> 910\n",
            "Epoch --> 911\n",
            "Epoch --> 912\n",
            "Epoch --> 913\n",
            "Epoch --> 914\n",
            "Epoch --> 915\n",
            "Epoch --> 916\n",
            "Epoch --> 917\n",
            "Epoch --> 918\n",
            "Epoch --> 919\n",
            "Epoch --> 920\n",
            "Epoch --> 921\n",
            "Epoch --> 922\n",
            "Epoch --> 923\n",
            "Epoch --> 924\n",
            "Epoch --> 925\n",
            "Epoch --> 926\n",
            "Epoch --> 927\n",
            "Epoch --> 928\n",
            "Epoch --> 929\n",
            "Epoch --> 930\n",
            "Epoch --> 931\n",
            "Epoch --> 932\n",
            "Epoch --> 933\n",
            "Epoch --> 934\n",
            "Epoch --> 935\n",
            "Epoch --> 936\n",
            "Epoch --> 937\n",
            "Epoch --> 938\n",
            "Epoch --> 939\n",
            "Epoch --> 940\n",
            "Epoch --> 941\n",
            "Epoch --> 942\n",
            "Epoch --> 943\n",
            "Epoch --> 944\n",
            "Epoch --> 945\n",
            "Epoch --> 946\n",
            "Epoch --> 947\n",
            "Epoch --> 948\n",
            "Epoch --> 949\n",
            "Epoch --> 950\n",
            "Epoch --> 951\n",
            "Epoch --> 952\n",
            "Epoch --> 953\n",
            "Epoch --> 954\n",
            "Epoch --> 955\n",
            "Epoch --> 956\n",
            "Epoch --> 957\n",
            "Epoch --> 958\n",
            "Epoch --> 959\n",
            "Epoch --> 960\n",
            "Epoch --> 961\n",
            "Epoch --> 962\n",
            "Epoch --> 963\n",
            "Epoch --> 964\n",
            "Epoch --> 965\n",
            "Epoch --> 966\n",
            "Epoch --> 967\n",
            "Epoch --> 968\n",
            "Epoch --> 969\n",
            "Epoch --> 970\n",
            "Epoch --> 971\n",
            "Epoch --> 972\n",
            "Epoch --> 973\n",
            "Epoch --> 974\n",
            "Epoch --> 975\n",
            "Epoch --> 976\n",
            "Epoch --> 977\n",
            "Epoch --> 978\n",
            "Epoch --> 979\n",
            "Epoch --> 980\n",
            "Epoch --> 981\n",
            "Epoch --> 982\n",
            "Epoch --> 983\n",
            "Epoch --> 984\n",
            "Epoch --> 985\n",
            "Epoch --> 986\n",
            "Epoch --> 987\n",
            "Epoch --> 988\n",
            "Epoch --> 989\n",
            "Epoch --> 990\n",
            "Epoch --> 991\n",
            "Epoch --> 992\n",
            "Epoch --> 993\n",
            "Epoch --> 994\n",
            "Epoch --> 995\n",
            "Epoch --> 996\n",
            "Epoch --> 997\n",
            "Epoch --> 998\n",
            "Epoch --> 999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final layer supervised training"
      ],
      "metadata": {
        "id": "ihk6TlJYfMdN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BioClassifier(nn.Module):\n",
        "    def __init__(self, W_unsupervised, layers_config, out_features):\n",
        "        super(BioClassifier, self).__init__()\n",
        "\n",
        "        self.W_unsupervised = nn.Parameter(W_unsupervised.T, requires_grad=False)  # Transpose to shape (784, hidden_dim)\n",
        "\n",
        "        self.layers = nn.Sequential()\n",
        "\n",
        "        # Input size is the number of neurons in unsupervised weights\n",
        "        input_size = W_unsupervised.size(0)\n",
        "\n",
        "        # Create multiple hidden layers as per `layers_config`\n",
        "        for idx, layer_size in enumerate(layers_config):\n",
        "            self.layers.add_module(f'linear_{idx}', nn.Linear(input_size, layer_size))\n",
        "            self.layers.add_module(f'batchnorm_{idx}', nn.BatchNorm1d(layer_size))\n",
        "            self.layers.add_module(f'relu_{idx}', nn.ReLU())\n",
        "            input_size = layer_size\n",
        "\n",
        "        # Output layer\n",
        "        self.output_layer = nn.Linear(input_size, out_features)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward through unsupervised weights\n",
        "        x = torch.matmul(x, self.W_unsupervised)  # Resulting shape should be (batch_size, input_dim)\n",
        "\n",
        "        # Forward through the hidden layers\n",
        "        x = self.layers(x)\n",
        "\n",
        "        # Output layer\n",
        "        x = self.output_layer(x)\n",
        "        return self.softmax(x)\n",
        "\n",
        "\n",
        "def train_bio_classifier(W_unsupervised, train_loader, val_loader, correct_img_bzs, data_name, layers_config, out_features, n_epochs, batch_size, anti_hebbian_learning_strength,\n",
        "                       lebesgue_norm, rank):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Clone initial unsupervised weights for verification\n",
        "    W_initial = W_unsupervised.clone().detach()\n",
        "    W_unsupervised.requires_grad = False\n",
        "\n",
        "    # Initialize BioClassifier with frozen W_unsupervised and multiple layers\n",
        "    model = BioClassifier(W_unsupervised, layers_config, out_features).to(device)\n",
        "\n",
        "    # Define the loss function\n",
        "    criterion = nn.CrossEntropyLoss().to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    # Scheduler setup\n",
        "    scheduler = MultiStepLR(optimizer, milestones=[100, 150, 200, 250], gamma=0.5)\n",
        "\n",
        "    # Lists for logging\n",
        "    train_loss_log, val_loss_log, train_acc_log, val_acc_log = [], [], [], []\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()\n",
        "        total_loss, correct = 0, 0\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "            if images.shape[0] != correct_img_bzs:\n",
        "                print(f\"Batch scartato per dimensione errata: {images.shape}\")\n",
        "                continue\n",
        "\n",
        "            images = images.view(batch_size, -1).to(device)  # Flatten images to (batch_size, img_sz)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backpropagation\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item() * images.size(0)\n",
        "            correct += (outputs.argmax(1) == labels).sum().item()\n",
        "\n",
        "        # Calculate average loss and accuracy for the epoch\n",
        "        train_loss = total_loss / len(train_loader.dataset)\n",
        "        train_accuracy = correct / len(train_loader.dataset)\n",
        "\n",
        "        # Store training logs\n",
        "        train_loss_log.append(train_loss)\n",
        "        train_acc_log.append(train_accuracy)\n",
        "\n",
        "        # Validation every epochs\n",
        "        val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
        "        val_loss_log.append(val_loss)\n",
        "        val_acc_log.append(val_acc)\n",
        "        print(f\"Epoch [{epoch+1}/{n_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Train Acc: {train_accuracy:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "        # Scheduler step\n",
        "        scheduler.step()\n",
        "\n",
        "    # Verify unsupervised weights remain unchanged\n",
        "    if torch.equal(W_unsupervised, W_initial):\n",
        "        print(\"Unsupervised weights remained unchanged during supervised training.\")\n",
        "    else:\n",
        "        print(\"Warning: Unsupervised weights were modified during supervised training.\")\n",
        "\n",
        "\n",
        "    # Plotting\n",
        "    # Dizionario dei nomi dei dataset\n",
        "    data_name_dict = {1: \"MNIST\", 2: \"CIFAR10\", 3: \"FashionMNIST\"}\n",
        "    dataset_name = data_name_dict.get(data_name, \"UnknownDataset\")\n",
        "\n",
        "    # Plot Loss\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.plot(np.linspace(0, len(val_loss_log), len(val_loss_log)), train_loss_log, '.-', label='Train Loss', color='red', lw=2, alpha=0.5)\n",
        "    plt.plot(np.linspace(0, len(val_loss_log), len(val_loss_log)), val_loss_log, '.-', label='Val Loss', color='blue', lw=2, alpha=0.5)\n",
        "    plt.title(f'Training and Validation Loss for {n_hidden} hidden units on {dataset_name}')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.savefig(f'output/{dataset_name}_biolinear_{n_hidden}hu_{out_features}out_{n_epochs}ep_ahls{anti_hebbian_learning_strength}_lnorm{lebesgue_norm}_rank{rank}_loss.png')\n",
        "    plt.show()\n",
        "\n",
        "    # Plot Accuracy\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.plot(np.linspace(0, len(val_loss_log), len(val_loss_log)), train_acc_log, '.-', label='Train Accuracy', color='red', lw=2, alpha=0.5)\n",
        "    plt.plot(np.linspace(0, len(val_loss_log), len(val_loss_log)), val_acc_log, '.-', label='Val Accuracy', color='blue', lw=2, alpha=0.5)\n",
        "    plt.title(f'Training and Validation Accuracy for {n_hidden} hidden units on {dataset_name}')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.savefig(f'content/{dataset_name}_biolinear_{n_hidden}hu_{out_features}out_{n_epochs}ep_ahls{anti_hebbian_learning_strength}_lnorm{lebesgue_norm}_rank{rank}_acc.png')\n",
        "    plt.show()\n",
        "\n",
        "    return train_loss_log, val_loss_log, train_acc_log, val_acc_log\n",
        "\n",
        "\n",
        "def validate(model, val_loader, criterion, device):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # Disable gradient calculations for validation\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images = images.view(images.size(0), -1).to(device)  # Flatten images and move to device\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Calculate accuracy\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    # Calculate average loss and accuracy\n",
        "    avg_loss = total_loss / len(val_loader)\n",
        "    accuracy = correct / total\n",
        "\n",
        "    return avg_loss, accuracy"
      ],
      "metadata": {
        "id": "Tm8RGPJnfMlg"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Draw weights"
      ],
      "metadata": {
        "id": "PMTVl1X60rE3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "f229c09c"
      },
      "outputs": [],
      "source": [
        "def draw_weights(synapses, Kx, Ky, ax=None):\n",
        "    # synapses: the weights\n",
        "    Kw = int(np.sqrt(synapses.shape[1]//3)) # i.e. 32\n",
        "    yy=0\n",
        "    HM=np.zeros((Kw*Ky, Kw*Kx, 3))\n",
        "    for y in range(Ky):\n",
        "        for x in range(Kx):\n",
        "            HM[y*Kw:(y+1)*Kw,x*Kw:(x+1)*Kw]=synapses[yy,:Kw*Kw*3].reshape(Kw, Kw, 3)\n",
        "            yy += 1\n",
        "\n",
        "    nc=np.amax(np.absolute(HM))\n",
        "    tmp = (HM-HM.min())\n",
        "    tmp /= tmp.max()\n",
        "    tmp *= 255\n",
        "    tmp = tmp.astype(np.uint8)\n",
        "    if ax is not None:\n",
        "        im = ax.imshow(tmp)\n",
        "        ax.axis('off')\n",
        "    else:\n",
        "        plt.clf()\n",
        "        im=plt.imshow(tmp.astype(np.uint8))\n",
        "        plt.axis('off')\n",
        "    fig.canvas.draw()\n",
        "\n",
        "# %matplotlib inline\n",
        "#fig=plt.figure(figsize=(10, 7))\n",
        "#draw_weights(synapses, 20, 10) # .to('cpu')\n",
        "#plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Multi layer linear bio classifier**\n",
        "\n",
        "The Multilayer Pipeline Bio Learner extends the single-layer model into multiple layers, forming a sequential processing pipeline. Each layer is trained independently using the same biologically inspired learning mechanism.\n",
        "The output of one layer becomes the input for the next, allowing for progressively more complex feature extraction. Finally, a fully connected layer is added on top, trained using backpropagation to specialize the model for a specific task (image classification) or prediction.\n",
        "\n",
        "![Multilayer Image](https://raw.githubusercontent.com/isottongloria/PMLS_Bio-Learning/main/multilayer.png)\n"
      ],
      "metadata": {
        "id": "ijDwDyff3MZC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# BioClassifier: the supervised classifier model with frozen unsupervised weights W\n",
        "class BioInference(nn.Module):\n",
        "    def __init__(self, W, out_features=10, n=4.5, beta=0.01):\n",
        "        super().__init__()\n",
        "\n",
        "        self.W = W.transpose(0, 1)\n",
        "        self.n = n\n",
        "        self.beta = beta\n",
        "\n",
        "        # Linear layer for supervised weights S (from hidden to output)\n",
        "        self.S = nn.Linear(W.size(0), out_features, bias=False)  # S learns during supervised phase\n",
        "\n",
        "    def forward(self, v):\n",
        "        # Move input v to the same device as self.W\n",
        "        v = v.to(self.W.device)\n",
        "\n",
        "        # Apply the frozen weights W\n",
        "        Wv = torch.matmul(v, self.W)  # (batch_sz, n_filters)\n",
        "        h = F.relu(Wv) ** self.n  # Apply nonlinearity with n exponent\n",
        "        Sh = self.S(h)  # Supervised layer S\n",
        "        c = torch.tanh(self.beta * Sh)  # Final output with tanh\n",
        "        return c\n",
        "\n",
        "\n",
        "def multi_layer_unsupervised_bio_learning(train_dataset, layers_config, n_epochs, batch_size,\n",
        "                                          learning_rate, precision, anti_hebbian_learning_strength,\n",
        "                                          lebesgue_norm, rank):\n",
        "    \"\"\"\n",
        "    Trains a multi-layer unsupervised model using a layer-wise approach.\n",
        "\n",
        "    Parameters:\n",
        "    - train_dataset: Input dataset (torch.Tensor), where each row is a training example.\n",
        "    - layers_config: A list with the number of hidden units for each layer.\n",
        "    - Other parameters are the same as for the unsupervised_bio_learning_layer function.\n",
        "\n",
        "    Returns:\n",
        "    - synapses_list: List of trained weights for each layer.\n",
        "    \"\"\"\n",
        "\n",
        "    synapses_list = []\n",
        "\n",
        "    # The number of input units for the current layer is the number of features from the previous layer\n",
        "    input_data = torch.stack([data[0].flatten() for data in train_dataset])\n",
        "    n_input = input_data.shape[1]\n",
        "    input_data = input_data.to(device)\n",
        "\n",
        "    for idx in range(len(layers_config)):\n",
        "\n",
        "        # number of output neurons/hidden layers\n",
        "        n_hidden = layers_config[idx]\n",
        "        print(f\"Training layer {idx + 1} with {n_hidden} hidden units.\")\n",
        "        print(f\"Training with {n_input} input units.\")\n",
        "\n",
        "        # compute the unsupervised weights\n",
        "        synapses = unsupervised_bio_learning(train_dataset,initial_synapses=None, n_hidden=n_hidden, n_epochs=n_epochs, batch_size=batch_size,\n",
        "                                              learning_rate=learning_rate, precision=precision, anti_hebbian_learning_strength=anti_hebbian_learning_strength,\n",
        "                                              lebesgue_norm=lebesgue_norm, rank=rank, skip=1)\n",
        "\n",
        "        synapses_list.append(synapses)\n",
        "\n",
        "        # Do a forward pass to infer the next input == 'train_dataset'\n",
        "        model = BioInference(synapses, out_features=n_hidden).to(device)\n",
        "        outputs = model(input_data)\n",
        "\n",
        "        n_input = outputs.shape[1]  # Pass the output to the next layer\n",
        "        print('next input', n_input)\n",
        "        restored_dataset = TensorDataset(outputs)   #.view(-1, 1, 28, 28)\n",
        "\n",
        "        # Update variables\n",
        "        train_dataset = restored_dataset\n",
        "        input_data = torch.stack([data[0].flatten().to(device) for data in train_dataset])\n",
        "\n",
        "    return synapses_list  # Return the list of all layers' weights\n"
      ],
      "metadata": {
        "id": "PzmnbcKw2UL3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def multi_layer_unsupervised_bio_learning(train_dataset, layers_config, n_epochs, batch_size,\n",
        "                                          learning_rate, precision, anti_hebbian_learning_strength,\n",
        "                                          lebesgue_norm, rank):\n",
        "    \"\"\"\n",
        "    Trains a multi-layer unsupervised model using a layer-wise approach.\n",
        "\n",
        "    Parameters:\n",
        "    - train_dataset: Input dataset (torch.Tensor), where each row is a training example.\n",
        "    - layers_config: A list with the number of hidden units for each layer.\n",
        "    - Other parameters are the same as for the unsupervised_bio_learning_layer function.\n",
        "\n",
        "    Returns:\n",
        "    - synapses_list: List of trained weights for each layer.\n",
        "    \"\"\"\n",
        "\n",
        "    synapses_list = []\n",
        "\n",
        "    # Prepare initial input data on GPU\n",
        "    input_data = torch.stack([data[0].flatten() for data in train_dataset]).to(device)\n",
        "    n_input = input_data.shape[1]\n",
        "\n",
        "    for idx in range(len(layers_config)):\n",
        "        # Number of output neurons/hidden units\n",
        "        n_hidden = layers_config[idx]\n",
        "        print(f\"Training layer {idx + 1} with {n_hidden} hidden units.\")\n",
        "        print(f\"Training with {n_input} input units.\")\n",
        "\n",
        "        # Compute the unsupervised weights\n",
        "        synapses = unsupervised_bio_learning(\n",
        "            train_dataset,\n",
        "            initial_synapses=None,\n",
        "            n_hidden=n_hidden,\n",
        "            n_epochs=n_epochs,\n",
        "            batch_size=batch_size,\n",
        "            learning_rate=learning_rate,\n",
        "            precision=precision,\n",
        "            anti_hebbian_learning_strength=anti_hebbian_learning_strength,\n",
        "            lebesgue_norm=lebesgue_norm,\n",
        "            rank=rank,\n",
        "            skip=1\n",
        "        )\n",
        "\n",
        "        synapses_list.append(synapses)\n",
        "\n",
        "        # Perform forward pass for the next input (inference)\n",
        "        with torch.no_grad():  # Disable gradient computation for inference\n",
        "            model = BioInference(synapses, out_features=n_hidden).to(device)\n",
        "            outputs = model(input_data)\n",
        "\n",
        "        # Free up the memory of previous input data\n",
        "        del input_data\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        # Update input for the next layer\n",
        "        n_input = outputs.shape[1]\n",
        "        print('next input', n_input)\n",
        "\n",
        "        # Move output to CPU to free up GPU memory before the next iteration\n",
        "        outputs = outputs.to('cpu')\n",
        "        restored_dataset = TensorDataset(outputs)\n",
        "\n",
        "        # Free the GPU memory of the model\n",
        "        del model\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        # Update input data for the next iteration\n",
        "        train_dataset = restored_dataset\n",
        "        input_data = torch.stack([data[0].flatten() for data in train_dataset]).to(device)\n",
        "\n",
        "    train_dataset_return = train_dataset\n",
        "    return synapses_list, train_dataset_return  # Return the list of all layers' weights\n"
      ],
      "metadata": {
        "id": "P6Io2gYkh3ZF"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FashionMNIST dataset"
      ],
      "metadata": {
        "id": "rrLWoHsfeVjt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fixed parameters in common to all unsupervised layers\n",
        "lebesgue_norm = 2\n",
        "rank = 2\n",
        "n_epochs_unsupervised = 5\n",
        "batch_size = 10\n",
        "learning_rate = 0.01\n",
        "precision = 0.1\n",
        "anti_hebbian_learning_strength = 0.4\n",
        "layers_config = [1000, 500, 100] # number of output neurons for each unsupervised layer\n",
        "\n",
        "# Output aesthetics parameters\n",
        "skip = 1\n",
        "Kx = 10\n",
        "Ky = 10\n",
        "\n",
        "synapses_list,train_dataset_final = multi_layer_unsupervised_bio_learning(train_dataset, layers_config, n_epochs_unsupervised, batch_size,\n",
        "                                          learning_rate, precision, anti_hebbian_learning_strength,\n",
        "                                          lebesgue_norm, rank)"
      ],
      "metadata": {
        "id": "6kHGPg7Q3O9s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3e46b61-eec5-447d-b051-d49815e939d1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training layer 1 with 1000 hidden units.\n",
            "Training with 784 input units.\n",
            "Epoch --> 0\n",
            "Epoch --> 1\n",
            "Epoch --> 2\n",
            "Epoch --> 3\n",
            "Epoch --> 4\n",
            "next input 1000\n",
            "Training layer 2 with 500 hidden units.\n",
            "Training with 1000 input units.\n",
            "Epoch --> 0\n",
            "Epoch --> 1\n",
            "Epoch --> 2\n",
            "Epoch --> 3\n",
            "Epoch --> 4\n",
            "next input 500\n",
            "Training layer 3 with 100 hidden units.\n",
            "Training with 500 input units.\n",
            "Epoch --> 0\n",
            "Epoch --> 1\n",
            "Epoch --> 2\n",
            "Epoch --> 3\n",
            "Epoch --> 4\n",
            "next input 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#fig=plt.figure(figsize=(10, 7))\n",
        "#draw_weights(synapses_list[2].to('cpu'), 10, 10)\n",
        "#plt.tight_layout()"
      ],
      "metadata": {
        "id": "Nf5sjuvvglL3"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fixed parameters for supervised learning\n",
        "out_features = 10\n",
        "n_epochs_supervised = 2\n",
        "batch_size_supervised = 28\n",
        "correct_img_bzs = 28\n",
        "n_hidden = layers_config[-1]\n",
        "\n",
        "# Supervised learning phase\n",
        "train_loss_log, val_loss_log, train_acc_log, val_acc_log = train_bio_classifier(\n",
        "    W_unsupervised=synapses_list[0],\n",
        "    train_loader=train_loader,\n",
        "    val_loader=test_loader,\n",
        "    correct_img_bzs=correct_img_bzs,\n",
        "    data_name=data_name,\n",
        "    layers_config = layers_config,\n",
        "    out_features=out_features,\n",
        "    n_epochs=n_epochs_supervised,\n",
        "    batch_size=batch_size_supervised,\n",
        "    anti_hebbian_learning_strength=anti_hebbian_learning_strength,\n",
        "    lebesgue_norm=lebesgue_norm,\n",
        "    rank=rank\n",
        ")"
      ],
      "metadata": {
        "id": "XgUdJXqoMsfb",
        "outputId": "be6d0a6d-c047-470e-e087-3a4e9da0772b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch scartato per dimensione errata: torch.Size([20, 784])\n",
            "Epoch [1/2], Train Loss: 2.0088, Val Loss: 2.1367, Train Acc: 0.4610, Val Acc: 0.3247\n",
            "Batch scartato per dimensione errata: torch.Size([20, 784])\n",
            "Epoch [2/2], Train Loss: 1.9168, Val Loss: 2.0413, Train Acc: 0.5552, Val Acc: 0.4067\n",
            "Unsupervised weights remained unchanged during supervised training.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/output/FashionMNIST_biolinear_100hu_10out_2ep_ahls0.4_lnorm2_rank2_loss.png'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-72e2f8722d47>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Supervised learning phase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m train_loss_log, val_loss_log, train_acc_log, val_acc_log = train_bio_classifier(\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mW_unsupervised\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynapses_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mtrain_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-2be840ef9138>\u001b[0m in \u001b[0;36mtrain_bio_classifier\u001b[0;34m(W_unsupervised, train_loader, val_loader, correct_img_bzs, data_name, layers_config, out_features, n_epochs, batch_size, anti_hebbian_learning_strength, lebesgue_norm, rank)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'output/{dataset_name}_biolinear_{n_hidden}hu_{out_features}out_{n_epochs}ep_ahls{anti_hebbian_learning_strength}_lnorm{lebesgue_norm}_rank{rank}_loss.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36msavefig\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1117\u001b[0m     \u001b[0;31m# savefig default implementation has no return, so mypy is unhappy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;31m# presumably this is here because subclasses can return?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1119\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[func-returns-value]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1120\u001b[0m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_idle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Need this if 'transparent=True', to reset colors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36msavefig\u001b[0;34m(self, fname, transparent, **kwargs)\u001b[0m\n\u001b[1;32m   3388\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0max\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3389\u001b[0m                     \u001b[0m_recursively_make_axes_transparent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3390\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3392\u001b[0m     def ginput(self, n=1, timeout=30, show_clicks=True,\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[1;32m   2185\u001b[0m                 \u001b[0;31m# force the figure dpi to 72), so we need to set it again here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2186\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setattr_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2187\u001b[0;31m                     result = print_method(\n\u001b[0m\u001b[1;32m   2188\u001b[0m                         \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2189\u001b[0m                         \u001b[0mfacecolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfacecolor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   2041\u001b[0m                 \"bbox_inches_restore\"}\n\u001b[1;32m   2042\u001b[0m             \u001b[0mskip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptional_kws\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2043\u001b[0;31m             print_method = functools.wraps(meth)(lambda *args, **kwargs: meth(\n\u001b[0m\u001b[1;32m   2044\u001b[0m                 *args, **{k: v for k, v in kwargs.items() if k not in skip}))\n\u001b[1;32m   2045\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Let third-parties do as they see fit.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mprint_png\u001b[0;34m(self, filename_or_obj, metadata, pil_kwargs)\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0;34m*\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincluding\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0;34m'Software'\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m         \"\"\"\n\u001b[0;32m--> 497\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_print_pil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename_or_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"png\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpil_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprint_to_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36m_print_pil\u001b[0;34m(self, filename_or_obj, fmt, pil_kwargs, metadata)\u001b[0m\n\u001b[1;32m    444\u001b[0m         \"\"\"\n\u001b[1;32m    445\u001b[0m         \u001b[0mFigureCanvasAgg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         mpl.image.imsave(\n\u001b[0m\u001b[1;32m    447\u001b[0m             \u001b[0mfilename_or_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer_rgba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfmt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morigin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"upper\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m             dpi=self.figure.dpi, metadata=metadata, pil_kwargs=pil_kwargs)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mimsave\u001b[0;34m(fname, arr, vmin, vmax, cmap, format, origin, dpi, metadata, pil_kwargs)\u001b[0m\n\u001b[1;32m   1654\u001b[0m         \u001b[0mpil_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"format\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1655\u001b[0m         \u001b[0mpil_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dpi\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdpi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1656\u001b[0;31m         \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpil_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   2561\u001b[0m                 \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r+b\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2562\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2563\u001b[0;31m                 \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w+b\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2564\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2565\u001b[0m             \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIO\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/output/FashionMNIST_biolinear_100hu_10out_2ep_ahls0.4_lnorm2_rank2_loss.png'"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "22BKckHFimMi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}