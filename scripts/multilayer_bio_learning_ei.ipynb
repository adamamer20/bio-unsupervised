{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMB1vCPUpf/62G4S2nz4U4+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isottongloria/PMLS_Bio-Learning/blob/main/scripts/multilayer_bio_learning_ei.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Unsupervised learning by competing hidden units**\n",
        "\n",
        " ### Author\n",
        "\n",
        " - Gloria Isotton\n",
        " - Master degree in Physics of data\n",
        "\n",
        "### Introduction\n",
        "\n",
        "In this notebook, we extend the approach presented in the paper to develop a more sophisticated unsupervised learning framework capable of training a **multi-layer network**. The goal is to leverage the unsupervised learning paradigm in a more complex network structure while incorporating **static excitatory and inhibitory synaptic connections** to improve the realism and performance of the model.\n",
        "\n",
        "The extension involves multiple layers of neurons, where each layer learns independently in an unsupervised manner, with connections between layers being modulated by excitatory and inhibitory synapses. This setup mimics the dynamics of biological neural networks more closely, where the balance between excitation and inhibition plays a crucial role in network stability, learning efficiency, and the emergence of meaningful representations.\n",
        "\n",
        "In the following sections, we will present the modifications made to the original framework, explain the design choices for synaptic regulation, and showcase the performance of the multi-layer model on several benchmark datasets. The code and the experiments demonstrate how unsupervised learning can be scaled to deeper architectures while preserving biological plausibility through synaptic balance."
      ],
      "metadata": {
        "id": "VwnS7783pNv8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing dependencies\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from PIL import Image\n",
        "from torch import nn,save,load\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split, TensorDataset\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import ExponentialLR\n",
        "from torch.optim.lr_scheduler import MultiStepLR\n",
        "import os\n"
      ],
      "metadata": {
        "id": "7jIoEBcMqPE9"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cuda Parameters\n",
        "use_cuda = torch.cuda.is_available()\n",
        "torch.cuda.empty_cache()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
      ],
      "metadata": {
        "id": "aPVUYk6KqiUu"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset loading\n",
        "Variable containing the dataset name `data_name`:\n",
        "- `1` : MNIST https://pytorch.org/vision/stable/generated/torchvision.datasets.MNIST.html?ref=hackernoon.com#torchvision.datasets.MNIST <br>\n",
        "- `2` : CIFAR10 https://pytorch.org/vision/stable/generated/torchvision.datasets.CIFAR10.html?ref=hackernoon.com#torchvision.datasets.CIFAR10<br>\n",
        "- `3` : FashionMNIST https://pytorch.org/vision/stable/generated/torchvision.datasets.FashionMNIST.html?ref=hackernoon.com#torchvision.datasets.FashionMNIST<br>"
      ],
      "metadata": {
        "id": "BHl0S9nFqUc2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_name=3\n",
        "\n",
        "####### MNIST dataset ########\n",
        "if data_name == 1:\n",
        "    transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                   transforms.Lambda(lambda x: x.view(-1))])\n",
        "    full_train_dataset = datasets.MNIST(root=\"data\", download=True, train=True, transform=transform)\n",
        "\n",
        "    train_size = 50000\n",
        "    val_size = 10000\n",
        "    train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=28, shuffle=True)\n",
        "    test_loader = DataLoader(val_dataset, batch_size=28, shuffle=False)\n",
        "\n",
        "    print(f\"Training set size: {len(train_loader.dataset)}\")\n",
        "    print(f\"Test set size: {len(test_loader.dataset)}\")\n",
        "\n",
        "\n",
        "####### CIFAR10 dataset ########\n",
        "if data_name == 2:\n",
        "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "                                   transforms.Lambda(lambda x: x.view(-1))])\n",
        "    full_train_dataset = datasets.CIFAR10(root=\"data\", download=True, train=True, transform=transform)\n",
        "\n",
        "    train_size = 40000\n",
        "    val_size = 10000\n",
        "    train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "    test_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    print(f\"Training set size: {len(train_loader.dataset)}\")\n",
        "    print(f\"Test set size: {len(test_loader.dataset)}\")\n",
        "\n",
        "\n",
        "\n",
        "####### FASHION MNIST dataset ########\n",
        "if data_name == 3:\n",
        "    transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,), (0.5,)),\n",
        "                                   transforms.Lambda(lambda x: x.view(-1))])\n",
        "\n",
        "    full_train_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True,download=True, transform=transform)\n",
        "    train_size = 50000\n",
        "    val_size = 10000\n",
        "    train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=28,shuffle=True)\n",
        "    test_loader = torch.utils.data.DataLoader(val_dataset, batch_size=28,shuffle=False)\n",
        "\n",
        "    print(f\"Training set size: {len(train_loader.dataset)}\")\n",
        "    print(f\"Test set size: {len(test_loader.dataset)}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "qgye8y5RqSUq",
        "outputId": "a80fb38c-0909-4df3-b603-984521cb59e4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:01<00:00, 16.6MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 277kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.42M/4.42M [00:00<00:00, 4.94MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 4.77MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Training set size: 50000\n",
            "Test set size: 10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Single layer unsupervised training**\n",
        " Unlike standard artificial neural networks, this implementation respects Dale's Law, a fundamental principle in neuroscience.\n",
        "Dale's Law states that a single neuron cannot have both excitatory and inhibitory effects on its post-synaptic neurons. In other words, each neuron can only be:\n",
        "  - Excitatory: Increases the activity of the neurons it connects to, using positive synaptic weights.\n",
        "\n",
        "  - Inhibitory: Decreases the activity of the neurons it connects to, using negative synaptic weights."
      ],
      "metadata": {
        "id": "dLamDhDOALx_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def unsupervised_bio_learning_dale(train_dataset, initial_synapses, n_hidden=28, n_epochs=500, batch_size=100,\n",
        "                                   learning_rate=0.01, precision=0.1,\n",
        "                                   anti_hebbian_learning_strength=0.3,\n",
        "                                   lebesgue_norm=2, rank=5, skip=1,\n",
        "                                   exc_ratio=0.8):\n",
        "    \"\"\"\n",
        "    Unsupervised bio learning with Dale's law constraints.\n",
        "\n",
        "    Parameters:\n",
        "    - train_dataset: Input dataset (torch.Tensor), where each row is a training example.\n",
        "    - initial_synapses: Initial synapse weights or None.\n",
        "    - n_hidden: Number of hidden units (neurons).\n",
        "    - n_epochs: Number of epochs to train the model.\n",
        "    - batch_size: Size of the minibatch for weight updates.\n",
        "    - learning_rate: Initial learning rate that decreases over epochs.\n",
        "    - precision: Threshold to normalize the gradient to avoid very small updates.\n",
        "    - anti_hebbian_learning_strength: Strength of anti-Hebbian learning.\n",
        "    - lebesgue_norm: Parameter for the Lebesgue norm.\n",
        "    - rank: Number of hidden neurons penalized with anti-Hebbian learning.\n",
        "    - skip: Epoch interval for logging.\n",
        "    - exc_ratio: Ratio of excitatory neurons (0 < exc_ratio < 1).\n",
        "    \"\"\"\n",
        "\n",
        "    # Flatten the input data and determine input size\n",
        "    input_data = torch.stack([data[0].flatten() for data in train_dataset]).to(device)\n",
        "    n_input = input_data.shape[1]  # Number of input neurons\n",
        "\n",
        "    # Determine excitatory and inhibitory indices based on ratio\n",
        "    n_exc = int(n_hidden * exc_ratio)\n",
        "    n_inh = n_hidden - n_exc\n",
        "\n",
        "    # Create excitatory and inhibitory indices\n",
        "    exc_indices = torch.arange(n_exc, device=device)\n",
        "    inh_indices = torch.arange(n_exc, n_hidden, device=device)\n",
        "\n",
        "    # Initialize synapse weights with Dale's law constraints\n",
        "    synapses = torch.rand((n_hidden, n_input), dtype=torch.float, device=device)\n",
        "    synapses[exc_indices] = torch.abs(synapses[exc_indices])  # Ensure excitatory weights are positive\n",
        "    synapses[inh_indices] = -torch.abs(synapses[inh_indices])  # Ensure inhibitory weights are negative\n",
        "\n",
        "    # Use provided initial synapses if available\n",
        "    if initial_synapses is not None:\n",
        "        synapses = initial_synapses.to(device)\n",
        "        synapses[exc_indices] = torch.abs(synapses[exc_indices])\n",
        "        synapses[inh_indices] = -torch.abs(synapses[inh_indices])\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(n_epochs):\n",
        "        if (epoch % skip == 0):\n",
        "            print('Epoch -->', epoch)\n",
        "\n",
        "        eps = learning_rate * (1 - epoch / n_epochs)  # Decaying learning rate\n",
        "\n",
        "        # Reshuffle dataset and maintain consistent neuron index shuffling\n",
        "        perm = torch.randperm(input_data.shape[0], device=device)\n",
        "        shuffled_epoch_data = input_data[perm, :]\n",
        "        neuron_perm = torch.randperm(n_hidden, device=device)\n",
        "\n",
        "        # Shuffle indices to ensure consistent excitatory/inhibitory pairing\n",
        "        shuffled_exc_indices = exc_indices[neuron_perm[:n_exc]]\n",
        "        shuffled_inh_indices = inh_indices[neuron_perm[n_exc:]]\n",
        "\n",
        "        # Minibatch loop\n",
        "        for i in range(0, len(train_dataset), batch_size):\n",
        "            mini_batch = shuffled_epoch_data[i:i + batch_size, :].to(device)\n",
        "            mini_batch = mini_batch.transpose(0, 1)\n",
        "\n",
        "            # Compute currents\n",
        "            sign = torch.sign(synapses).to(device)\n",
        "            tot_input = torch.mm(sign * torch.abs(synapses).pow(lebesgue_norm - 1), mini_batch)\n",
        "\n",
        "            # Sort activations\n",
        "            y = torch.argsort(tot_input, dim=0).to(device)\n",
        "            yl = torch.zeros((n_hidden, batch_size)).to(device)\n",
        "\n",
        "            # Hebbian (max activation) and anti-Hebbian (penalize low activation)\n",
        "            yl[y[n_hidden - 1, :], torch.arange(batch_size, device=device)] = 1.0\n",
        "            yl[y[n_hidden - rank, :], torch.arange(batch_size, device=device)] = -anti_hebbian_learning_strength\n",
        "\n",
        "            # Compute the contribution of activations on the total input received\n",
        "            xx = torch.sum(torch.mul(yl, tot_input), 1)\n",
        "\n",
        "            # Weight update calculation\n",
        "            ds = torch.matmul(yl, torch.transpose(mini_batch, 0, 1)) - torch.mul(xx.reshape(xx.shape[0], 1).repeat(1, n_input), synapses)\n",
        "\n",
        "            # Normalize gradient\n",
        "            nc = torch.max(torch.abs(ds))\n",
        "            if nc < precision:\n",
        "                nc = precision\n",
        "\n",
        "            # Update synapse weights\n",
        "            synapses += torch.mul(torch.div(ds, nc), eps)\n",
        "\n",
        "            # Enforce Dale's law: excitatory weights remain positive, inhibitory weights remain negative\n",
        "            synapses[shuffled_exc_indices] = torch.abs(synapses[shuffled_exc_indices])\n",
        "            synapses[shuffled_inh_indices] = -torch.abs(synapses[shuffled_inh_indices])\n",
        "\n",
        "    return synapses\n"
      ],
      "metadata": {
        "id": "JTWU8yKXo6cj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "synapses = unsupervised_bio_learning_dale(train_dataset,initial_synapses=None, n_hidden=2000, n_epochs=1000, batch_size=100,\n",
        "                              learning_rate=0.01, precision=0.1,\n",
        "                              anti_hebbian_learning_strength=0.3,\n",
        "                              lebesgue_norm=2, rank=2, skip=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "CvHDg4WO0bqN",
        "outputId": "b7b2aa79-5141-49e9-88de-e0585359f935"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch --> 0\n",
            "Epoch --> 1\n",
            "Epoch --> 2\n",
            "Epoch --> 3\n",
            "Epoch --> 4\n",
            "Epoch --> 5\n",
            "Epoch --> 6\n",
            "Epoch --> 7\n",
            "Epoch --> 8\n",
            "Epoch --> 9\n",
            "Epoch --> 10\n",
            "Epoch --> 11\n",
            "Epoch --> 12\n",
            "Epoch --> 13\n",
            "Epoch --> 14\n",
            "Epoch --> 15\n",
            "Epoch --> 16\n",
            "Epoch --> 17\n",
            "Epoch --> 18\n",
            "Epoch --> 19\n",
            "Epoch --> 20\n",
            "Epoch --> 21\n",
            "Epoch --> 22\n",
            "Epoch --> 23\n",
            "Epoch --> 24\n",
            "Epoch --> 25\n",
            "Epoch --> 26\n",
            "Epoch --> 27\n",
            "Epoch --> 28\n",
            "Epoch --> 29\n",
            "Epoch --> 30\n",
            "Epoch --> 31\n",
            "Epoch --> 32\n",
            "Epoch --> 33\n",
            "Epoch --> 34\n",
            "Epoch --> 35\n",
            "Epoch --> 36\n",
            "Epoch --> 37\n",
            "Epoch --> 38\n",
            "Epoch --> 39\n",
            "Epoch --> 40\n",
            "Epoch --> 41\n",
            "Epoch --> 42\n",
            "Epoch --> 43\n",
            "Epoch --> 44\n",
            "Epoch --> 45\n",
            "Epoch --> 46\n",
            "Epoch --> 47\n",
            "Epoch --> 48\n",
            "Epoch --> 49\n",
            "Epoch --> 50\n",
            "Epoch --> 51\n",
            "Epoch --> 52\n",
            "Epoch --> 53\n",
            "Epoch --> 54\n",
            "Epoch --> 55\n",
            "Epoch --> 56\n",
            "Epoch --> 57\n",
            "Epoch --> 58\n",
            "Epoch --> 59\n",
            "Epoch --> 60\n",
            "Epoch --> 61\n",
            "Epoch --> 62\n",
            "Epoch --> 63\n",
            "Epoch --> 64\n",
            "Epoch --> 65\n",
            "Epoch --> 66\n",
            "Epoch --> 67\n",
            "Epoch --> 68\n",
            "Epoch --> 69\n",
            "Epoch --> 70\n",
            "Epoch --> 71\n",
            "Epoch --> 72\n",
            "Epoch --> 73\n",
            "Epoch --> 74\n",
            "Epoch --> 75\n",
            "Epoch --> 76\n",
            "Epoch --> 77\n",
            "Epoch --> 78\n",
            "Epoch --> 79\n",
            "Epoch --> 80\n",
            "Epoch --> 81\n",
            "Epoch --> 82\n",
            "Epoch --> 83\n",
            "Epoch --> 84\n",
            "Epoch --> 85\n",
            "Epoch --> 86\n",
            "Epoch --> 87\n",
            "Epoch --> 88\n",
            "Epoch --> 89\n",
            "Epoch --> 90\n",
            "Epoch --> 91\n",
            "Epoch --> 92\n",
            "Epoch --> 93\n",
            "Epoch --> 94\n",
            "Epoch --> 95\n",
            "Epoch --> 96\n",
            "Epoch --> 97\n",
            "Epoch --> 98\n",
            "Epoch --> 99\n",
            "Epoch --> 100\n",
            "Epoch --> 101\n",
            "Epoch --> 102\n",
            "Epoch --> 103\n",
            "Epoch --> 104\n",
            "Epoch --> 105\n",
            "Epoch --> 106\n",
            "Epoch --> 107\n",
            "Epoch --> 108\n",
            "Epoch --> 109\n",
            "Epoch --> 110\n",
            "Epoch --> 111\n",
            "Epoch --> 112\n",
            "Epoch --> 113\n",
            "Epoch --> 114\n",
            "Epoch --> 115\n",
            "Epoch --> 116\n",
            "Epoch --> 117\n",
            "Epoch --> 118\n",
            "Epoch --> 119\n",
            "Epoch --> 120\n",
            "Epoch --> 121\n",
            "Epoch --> 122\n",
            "Epoch --> 123\n",
            "Epoch --> 124\n",
            "Epoch --> 125\n",
            "Epoch --> 126\n",
            "Epoch --> 127\n",
            "Epoch --> 128\n",
            "Epoch --> 129\n",
            "Epoch --> 130\n",
            "Epoch --> 131\n",
            "Epoch --> 132\n",
            "Epoch --> 133\n",
            "Epoch --> 134\n",
            "Epoch --> 135\n",
            "Epoch --> 136\n",
            "Epoch --> 137\n",
            "Epoch --> 138\n",
            "Epoch --> 139\n",
            "Epoch --> 140\n",
            "Epoch --> 141\n",
            "Epoch --> 142\n",
            "Epoch --> 143\n",
            "Epoch --> 144\n",
            "Epoch --> 145\n",
            "Epoch --> 146\n",
            "Epoch --> 147\n",
            "Epoch --> 148\n",
            "Epoch --> 149\n",
            "Epoch --> 150\n",
            "Epoch --> 151\n",
            "Epoch --> 152\n",
            "Epoch --> 153\n",
            "Epoch --> 154\n",
            "Epoch --> 155\n",
            "Epoch --> 156\n",
            "Epoch --> 157\n",
            "Epoch --> 158\n",
            "Epoch --> 159\n",
            "Epoch --> 160\n",
            "Epoch --> 161\n",
            "Epoch --> 162\n",
            "Epoch --> 163\n",
            "Epoch --> 164\n",
            "Epoch --> 165\n",
            "Epoch --> 166\n",
            "Epoch --> 167\n",
            "Epoch --> 168\n",
            "Epoch --> 169\n",
            "Epoch --> 170\n",
            "Epoch --> 171\n",
            "Epoch --> 172\n",
            "Epoch --> 173\n",
            "Epoch --> 174\n",
            "Epoch --> 175\n",
            "Epoch --> 176\n",
            "Epoch --> 177\n",
            "Epoch --> 178\n",
            "Epoch --> 179\n",
            "Epoch --> 180\n",
            "Epoch --> 181\n",
            "Epoch --> 182\n",
            "Epoch --> 183\n",
            "Epoch --> 184\n",
            "Epoch --> 185\n",
            "Epoch --> 186\n",
            "Epoch --> 187\n",
            "Epoch --> 188\n",
            "Epoch --> 189\n",
            "Epoch --> 190\n",
            "Epoch --> 191\n",
            "Epoch --> 192\n",
            "Epoch --> 193\n",
            "Epoch --> 194\n",
            "Epoch --> 195\n",
            "Epoch --> 196\n",
            "Epoch --> 197\n",
            "Epoch --> 198\n",
            "Epoch --> 199\n",
            "Epoch --> 200\n",
            "Epoch --> 201\n",
            "Epoch --> 202\n",
            "Epoch --> 203\n",
            "Epoch --> 204\n",
            "Epoch --> 205\n",
            "Epoch --> 206\n",
            "Epoch --> 207\n",
            "Epoch --> 208\n",
            "Epoch --> 209\n",
            "Epoch --> 210\n",
            "Epoch --> 211\n",
            "Epoch --> 212\n",
            "Epoch --> 213\n",
            "Epoch --> 214\n",
            "Epoch --> 215\n",
            "Epoch --> 216\n",
            "Epoch --> 217\n",
            "Epoch --> 218\n",
            "Epoch --> 219\n",
            "Epoch --> 220\n",
            "Epoch --> 221\n",
            "Epoch --> 222\n",
            "Epoch --> 223\n",
            "Epoch --> 224\n",
            "Epoch --> 225\n",
            "Epoch --> 226\n",
            "Epoch --> 227\n",
            "Epoch --> 228\n",
            "Epoch --> 229\n",
            "Epoch --> 230\n",
            "Epoch --> 231\n",
            "Epoch --> 232\n",
            "Epoch --> 233\n",
            "Epoch --> 234\n",
            "Epoch --> 235\n",
            "Epoch --> 236\n",
            "Epoch --> 237\n",
            "Epoch --> 238\n",
            "Epoch --> 239\n",
            "Epoch --> 240\n",
            "Epoch --> 241\n",
            "Epoch --> 242\n",
            "Epoch --> 243\n",
            "Epoch --> 244\n",
            "Epoch --> 245\n",
            "Epoch --> 246\n",
            "Epoch --> 247\n",
            "Epoch --> 248\n",
            "Epoch --> 249\n",
            "Epoch --> 250\n",
            "Epoch --> 251\n",
            "Epoch --> 252\n",
            "Epoch --> 253\n",
            "Epoch --> 254\n",
            "Epoch --> 255\n",
            "Epoch --> 256\n",
            "Epoch --> 257\n",
            "Epoch --> 258\n",
            "Epoch --> 259\n",
            "Epoch --> 260\n",
            "Epoch --> 261\n",
            "Epoch --> 262\n",
            "Epoch --> 263\n",
            "Epoch --> 264\n",
            "Epoch --> 265\n",
            "Epoch --> 266\n",
            "Epoch --> 267\n",
            "Epoch --> 268\n",
            "Epoch --> 269\n",
            "Epoch --> 270\n",
            "Epoch --> 271\n",
            "Epoch --> 272\n",
            "Epoch --> 273\n",
            "Epoch --> 274\n",
            "Epoch --> 275\n",
            "Epoch --> 276\n",
            "Epoch --> 277\n",
            "Epoch --> 278\n",
            "Epoch --> 279\n",
            "Epoch --> 280\n",
            "Epoch --> 281\n",
            "Epoch --> 282\n",
            "Epoch --> 283\n",
            "Epoch --> 284\n",
            "Epoch --> 285\n",
            "Epoch --> 286\n",
            "Epoch --> 287\n",
            "Epoch --> 288\n",
            "Epoch --> 289\n",
            "Epoch --> 290\n",
            "Epoch --> 291\n",
            "Epoch --> 292\n",
            "Epoch --> 293\n",
            "Epoch --> 294\n",
            "Epoch --> 295\n",
            "Epoch --> 296\n",
            "Epoch --> 297\n",
            "Epoch --> 298\n",
            "Epoch --> 299\n",
            "Epoch --> 300\n",
            "Epoch --> 301\n",
            "Epoch --> 302\n",
            "Epoch --> 303\n",
            "Epoch --> 304\n",
            "Epoch --> 305\n",
            "Epoch --> 306\n",
            "Epoch --> 307\n",
            "Epoch --> 308\n",
            "Epoch --> 309\n",
            "Epoch --> 310\n",
            "Epoch --> 311\n",
            "Epoch --> 312\n",
            "Epoch --> 313\n",
            "Epoch --> 314\n",
            "Epoch --> 315\n",
            "Epoch --> 316\n",
            "Epoch --> 317\n",
            "Epoch --> 318\n",
            "Epoch --> 319\n",
            "Epoch --> 320\n",
            "Epoch --> 321\n",
            "Epoch --> 322\n",
            "Epoch --> 323\n",
            "Epoch --> 324\n",
            "Epoch --> 325\n",
            "Epoch --> 326\n",
            "Epoch --> 327\n",
            "Epoch --> 328\n",
            "Epoch --> 329\n",
            "Epoch --> 330\n",
            "Epoch --> 331\n",
            "Epoch --> 332\n",
            "Epoch --> 333\n",
            "Epoch --> 334\n",
            "Epoch --> 335\n",
            "Epoch --> 336\n",
            "Epoch --> 337\n",
            "Epoch --> 338\n",
            "Epoch --> 339\n",
            "Epoch --> 340\n",
            "Epoch --> 341\n",
            "Epoch --> 342\n",
            "Epoch --> 343\n",
            "Epoch --> 344\n",
            "Epoch --> 345\n",
            "Epoch --> 346\n",
            "Epoch --> 347\n",
            "Epoch --> 348\n",
            "Epoch --> 349\n",
            "Epoch --> 350\n",
            "Epoch --> 351\n",
            "Epoch --> 352\n",
            "Epoch --> 353\n",
            "Epoch --> 354\n",
            "Epoch --> 355\n",
            "Epoch --> 356\n",
            "Epoch --> 357\n",
            "Epoch --> 358\n",
            "Epoch --> 359\n",
            "Epoch --> 360\n",
            "Epoch --> 361\n",
            "Epoch --> 362\n",
            "Epoch --> 363\n",
            "Epoch --> 364\n",
            "Epoch --> 365\n",
            "Epoch --> 366\n",
            "Epoch --> 367\n",
            "Epoch --> 368\n",
            "Epoch --> 369\n",
            "Epoch --> 370\n",
            "Epoch --> 371\n",
            "Epoch --> 372\n",
            "Epoch --> 373\n",
            "Epoch --> 374\n",
            "Epoch --> 375\n",
            "Epoch --> 376\n",
            "Epoch --> 377\n",
            "Epoch --> 378\n",
            "Epoch --> 379\n",
            "Epoch --> 380\n",
            "Epoch --> 381\n",
            "Epoch --> 382\n",
            "Epoch --> 383\n",
            "Epoch --> 384\n",
            "Epoch --> 385\n",
            "Epoch --> 386\n",
            "Epoch --> 387\n",
            "Epoch --> 388\n",
            "Epoch --> 389\n",
            "Epoch --> 390\n",
            "Epoch --> 391\n",
            "Epoch --> 392\n",
            "Epoch --> 393\n",
            "Epoch --> 394\n",
            "Epoch --> 395\n",
            "Epoch --> 396\n",
            "Epoch --> 397\n",
            "Epoch --> 398\n",
            "Epoch --> 399\n",
            "Epoch --> 400\n",
            "Epoch --> 401\n",
            "Epoch --> 402\n",
            "Epoch --> 403\n",
            "Epoch --> 404\n",
            "Epoch --> 405\n",
            "Epoch --> 406\n",
            "Epoch --> 407\n",
            "Epoch --> 408\n",
            "Epoch --> 409\n",
            "Epoch --> 410\n",
            "Epoch --> 411\n",
            "Epoch --> 412\n",
            "Epoch --> 413\n",
            "Epoch --> 414\n",
            "Epoch --> 415\n",
            "Epoch --> 416\n",
            "Epoch --> 417\n",
            "Epoch --> 418\n",
            "Epoch --> 419\n",
            "Epoch --> 420\n",
            "Epoch --> 421\n",
            "Epoch --> 422\n",
            "Epoch --> 423\n",
            "Epoch --> 424\n",
            "Epoch --> 425\n",
            "Epoch --> 426\n",
            "Epoch --> 427\n",
            "Epoch --> 428\n",
            "Epoch --> 429\n",
            "Epoch --> 430\n",
            "Epoch --> 431\n",
            "Epoch --> 432\n",
            "Epoch --> 433\n",
            "Epoch --> 434\n",
            "Epoch --> 435\n",
            "Epoch --> 436\n",
            "Epoch --> 437\n",
            "Epoch --> 438\n",
            "Epoch --> 439\n",
            "Epoch --> 440\n",
            "Epoch --> 441\n",
            "Epoch --> 442\n",
            "Epoch --> 443\n",
            "Epoch --> 444\n",
            "Epoch --> 445\n",
            "Epoch --> 446\n",
            "Epoch --> 447\n",
            "Epoch --> 448\n",
            "Epoch --> 449\n",
            "Epoch --> 450\n",
            "Epoch --> 451\n",
            "Epoch --> 452\n",
            "Epoch --> 453\n",
            "Epoch --> 454\n",
            "Epoch --> 455\n",
            "Epoch --> 456\n",
            "Epoch --> 457\n",
            "Epoch --> 458\n",
            "Epoch --> 459\n",
            "Epoch --> 460\n",
            "Epoch --> 461\n",
            "Epoch --> 462\n",
            "Epoch --> 463\n",
            "Epoch --> 464\n",
            "Epoch --> 465\n",
            "Epoch --> 466\n",
            "Epoch --> 467\n",
            "Epoch --> 468\n",
            "Epoch --> 469\n",
            "Epoch --> 470\n",
            "Epoch --> 471\n",
            "Epoch --> 472\n",
            "Epoch --> 473\n",
            "Epoch --> 474\n",
            "Epoch --> 475\n",
            "Epoch --> 476\n",
            "Epoch --> 477\n",
            "Epoch --> 478\n",
            "Epoch --> 479\n",
            "Epoch --> 480\n",
            "Epoch --> 481\n",
            "Epoch --> 482\n",
            "Epoch --> 483\n",
            "Epoch --> 484\n",
            "Epoch --> 485\n",
            "Epoch --> 486\n",
            "Epoch --> 487\n",
            "Epoch --> 488\n",
            "Epoch --> 489\n",
            "Epoch --> 490\n",
            "Epoch --> 491\n",
            "Epoch --> 492\n",
            "Epoch --> 493\n",
            "Epoch --> 494\n",
            "Epoch --> 495\n",
            "Epoch --> 496\n",
            "Epoch --> 497\n",
            "Epoch --> 498\n",
            "Epoch --> 499\n",
            "Epoch --> 500\n",
            "Epoch --> 501\n",
            "Epoch --> 502\n",
            "Epoch --> 503\n",
            "Epoch --> 504\n",
            "Epoch --> 505\n",
            "Epoch --> 506\n",
            "Epoch --> 507\n",
            "Epoch --> 508\n",
            "Epoch --> 509\n",
            "Epoch --> 510\n",
            "Epoch --> 511\n",
            "Epoch --> 512\n",
            "Epoch --> 513\n",
            "Epoch --> 514\n",
            "Epoch --> 515\n",
            "Epoch --> 516\n",
            "Epoch --> 517\n",
            "Epoch --> 518\n",
            "Epoch --> 519\n",
            "Epoch --> 520\n",
            "Epoch --> 521\n",
            "Epoch --> 522\n",
            "Epoch --> 523\n",
            "Epoch --> 524\n",
            "Epoch --> 525\n",
            "Epoch --> 526\n",
            "Epoch --> 527\n",
            "Epoch --> 528\n",
            "Epoch --> 529\n",
            "Epoch --> 530\n",
            "Epoch --> 531\n",
            "Epoch --> 532\n",
            "Epoch --> 533\n",
            "Epoch --> 534\n",
            "Epoch --> 535\n",
            "Epoch --> 536\n",
            "Epoch --> 537\n",
            "Epoch --> 538\n",
            "Epoch --> 539\n",
            "Epoch --> 540\n",
            "Epoch --> 541\n",
            "Epoch --> 542\n",
            "Epoch --> 543\n",
            "Epoch --> 544\n",
            "Epoch --> 545\n",
            "Epoch --> 546\n",
            "Epoch --> 547\n",
            "Epoch --> 548\n",
            "Epoch --> 549\n",
            "Epoch --> 550\n",
            "Epoch --> 551\n",
            "Epoch --> 552\n",
            "Epoch --> 553\n",
            "Epoch --> 554\n",
            "Epoch --> 555\n",
            "Epoch --> 556\n",
            "Epoch --> 557\n",
            "Epoch --> 558\n",
            "Epoch --> 559\n",
            "Epoch --> 560\n",
            "Epoch --> 561\n",
            "Epoch --> 562\n",
            "Epoch --> 563\n",
            "Epoch --> 564\n",
            "Epoch --> 565\n",
            "Epoch --> 566\n",
            "Epoch --> 567\n",
            "Epoch --> 568\n",
            "Epoch --> 569\n",
            "Epoch --> 570\n",
            "Epoch --> 571\n",
            "Epoch --> 572\n",
            "Epoch --> 573\n",
            "Epoch --> 574\n",
            "Epoch --> 575\n",
            "Epoch --> 576\n",
            "Epoch --> 577\n",
            "Epoch --> 578\n",
            "Epoch --> 579\n",
            "Epoch --> 580\n",
            "Epoch --> 581\n",
            "Epoch --> 582\n",
            "Epoch --> 583\n",
            "Epoch --> 584\n",
            "Epoch --> 585\n",
            "Epoch --> 586\n",
            "Epoch --> 587\n",
            "Epoch --> 588\n",
            "Epoch --> 589\n",
            "Epoch --> 590\n",
            "Epoch --> 591\n",
            "Epoch --> 592\n",
            "Epoch --> 593\n",
            "Epoch --> 594\n",
            "Epoch --> 595\n",
            "Epoch --> 596\n",
            "Epoch --> 597\n",
            "Epoch --> 598\n",
            "Epoch --> 599\n",
            "Epoch --> 600\n",
            "Epoch --> 601\n",
            "Epoch --> 602\n",
            "Epoch --> 603\n",
            "Epoch --> 604\n",
            "Epoch --> 605\n",
            "Epoch --> 606\n",
            "Epoch --> 607\n",
            "Epoch --> 608\n",
            "Epoch --> 609\n",
            "Epoch --> 610\n",
            "Epoch --> 611\n",
            "Epoch --> 612\n",
            "Epoch --> 613\n",
            "Epoch --> 614\n",
            "Epoch --> 615\n",
            "Epoch --> 616\n",
            "Epoch --> 617\n",
            "Epoch --> 618\n",
            "Epoch --> 619\n",
            "Epoch --> 620\n",
            "Epoch --> 621\n",
            "Epoch --> 622\n",
            "Epoch --> 623\n",
            "Epoch --> 624\n",
            "Epoch --> 625\n",
            "Epoch --> 626\n",
            "Epoch --> 627\n",
            "Epoch --> 628\n",
            "Epoch --> 629\n",
            "Epoch --> 630\n",
            "Epoch --> 631\n",
            "Epoch --> 632\n",
            "Epoch --> 633\n",
            "Epoch --> 634\n",
            "Epoch --> 635\n",
            "Epoch --> 636\n",
            "Epoch --> 637\n",
            "Epoch --> 638\n",
            "Epoch --> 639\n",
            "Epoch --> 640\n",
            "Epoch --> 641\n",
            "Epoch --> 642\n",
            "Epoch --> 643\n",
            "Epoch --> 644\n",
            "Epoch --> 645\n",
            "Epoch --> 646\n",
            "Epoch --> 647\n",
            "Epoch --> 648\n",
            "Epoch --> 649\n",
            "Epoch --> 650\n",
            "Epoch --> 651\n",
            "Epoch --> 652\n",
            "Epoch --> 653\n",
            "Epoch --> 654\n",
            "Epoch --> 655\n",
            "Epoch --> 656\n",
            "Epoch --> 657\n",
            "Epoch --> 658\n",
            "Epoch --> 659\n",
            "Epoch --> 660\n",
            "Epoch --> 661\n",
            "Epoch --> 662\n",
            "Epoch --> 663\n",
            "Epoch --> 664\n",
            "Epoch --> 665\n",
            "Epoch --> 666\n",
            "Epoch --> 667\n",
            "Epoch --> 668\n",
            "Epoch --> 669\n",
            "Epoch --> 670\n",
            "Epoch --> 671\n",
            "Epoch --> 672\n",
            "Epoch --> 673\n",
            "Epoch --> 674\n",
            "Epoch --> 675\n",
            "Epoch --> 676\n",
            "Epoch --> 677\n",
            "Epoch --> 678\n",
            "Epoch --> 679\n",
            "Epoch --> 680\n",
            "Epoch --> 681\n",
            "Epoch --> 682\n",
            "Epoch --> 683\n",
            "Epoch --> 684\n",
            "Epoch --> 685\n",
            "Epoch --> 686\n",
            "Epoch --> 687\n",
            "Epoch --> 688\n",
            "Epoch --> 689\n",
            "Epoch --> 690\n",
            "Epoch --> 691\n",
            "Epoch --> 692\n",
            "Epoch --> 693\n",
            "Epoch --> 694\n",
            "Epoch --> 695\n",
            "Epoch --> 696\n",
            "Epoch --> 697\n",
            "Epoch --> 698\n",
            "Epoch --> 699\n",
            "Epoch --> 700\n",
            "Epoch --> 701\n",
            "Epoch --> 702\n",
            "Epoch --> 703\n",
            "Epoch --> 704\n",
            "Epoch --> 705\n",
            "Epoch --> 706\n",
            "Epoch --> 707\n",
            "Epoch --> 708\n",
            "Epoch --> 709\n",
            "Epoch --> 710\n",
            "Epoch --> 711\n",
            "Epoch --> 712\n",
            "Epoch --> 713\n",
            "Epoch --> 714\n",
            "Epoch --> 715\n",
            "Epoch --> 716\n",
            "Epoch --> 717\n",
            "Epoch --> 718\n",
            "Epoch --> 719\n",
            "Epoch --> 720\n",
            "Epoch --> 721\n",
            "Epoch --> 722\n",
            "Epoch --> 723\n",
            "Epoch --> 724\n",
            "Epoch --> 725\n",
            "Epoch --> 726\n",
            "Epoch --> 727\n",
            "Epoch --> 728\n",
            "Epoch --> 729\n",
            "Epoch --> 730\n",
            "Epoch --> 731\n",
            "Epoch --> 732\n",
            "Epoch --> 733\n",
            "Epoch --> 734\n",
            "Epoch --> 735\n",
            "Epoch --> 736\n",
            "Epoch --> 737\n",
            "Epoch --> 738\n",
            "Epoch --> 739\n",
            "Epoch --> 740\n",
            "Epoch --> 741\n",
            "Epoch --> 742\n",
            "Epoch --> 743\n",
            "Epoch --> 744\n",
            "Epoch --> 745\n",
            "Epoch --> 746\n",
            "Epoch --> 747\n",
            "Epoch --> 748\n",
            "Epoch --> 749\n",
            "Epoch --> 750\n",
            "Epoch --> 751\n",
            "Epoch --> 752\n",
            "Epoch --> 753\n",
            "Epoch --> 754\n",
            "Epoch --> 755\n",
            "Epoch --> 756\n",
            "Epoch --> 757\n",
            "Epoch --> 758\n",
            "Epoch --> 759\n",
            "Epoch --> 760\n",
            "Epoch --> 761\n",
            "Epoch --> 762\n",
            "Epoch --> 763\n",
            "Epoch --> 764\n",
            "Epoch --> 765\n",
            "Epoch --> 766\n",
            "Epoch --> 767\n",
            "Epoch --> 768\n",
            "Epoch --> 769\n",
            "Epoch --> 770\n",
            "Epoch --> 771\n",
            "Epoch --> 772\n",
            "Epoch --> 773\n",
            "Epoch --> 774\n",
            "Epoch --> 775\n",
            "Epoch --> 776\n",
            "Epoch --> 777\n",
            "Epoch --> 778\n",
            "Epoch --> 779\n",
            "Epoch --> 780\n",
            "Epoch --> 781\n",
            "Epoch --> 782\n",
            "Epoch --> 783\n",
            "Epoch --> 784\n",
            "Epoch --> 785\n",
            "Epoch --> 786\n",
            "Epoch --> 787\n",
            "Epoch --> 788\n",
            "Epoch --> 789\n",
            "Epoch --> 790\n",
            "Epoch --> 791\n",
            "Epoch --> 792\n",
            "Epoch --> 793\n",
            "Epoch --> 794\n",
            "Epoch --> 795\n",
            "Epoch --> 796\n",
            "Epoch --> 797\n",
            "Epoch --> 798\n",
            "Epoch --> 799\n",
            "Epoch --> 800\n",
            "Epoch --> 801\n",
            "Epoch --> 802\n",
            "Epoch --> 803\n",
            "Epoch --> 804\n",
            "Epoch --> 805\n",
            "Epoch --> 806\n",
            "Epoch --> 807\n",
            "Epoch --> 808\n",
            "Epoch --> 809\n",
            "Epoch --> 810\n",
            "Epoch --> 811\n",
            "Epoch --> 812\n",
            "Epoch --> 813\n",
            "Epoch --> 814\n",
            "Epoch --> 815\n",
            "Epoch --> 816\n",
            "Epoch --> 817\n",
            "Epoch --> 818\n",
            "Epoch --> 819\n",
            "Epoch --> 820\n",
            "Epoch --> 821\n",
            "Epoch --> 822\n",
            "Epoch --> 823\n",
            "Epoch --> 824\n",
            "Epoch --> 825\n",
            "Epoch --> 826\n",
            "Epoch --> 827\n",
            "Epoch --> 828\n",
            "Epoch --> 829\n",
            "Epoch --> 830\n",
            "Epoch --> 831\n",
            "Epoch --> 832\n",
            "Epoch --> 833\n",
            "Epoch --> 834\n",
            "Epoch --> 835\n",
            "Epoch --> 836\n",
            "Epoch --> 837\n",
            "Epoch --> 838\n",
            "Epoch --> 839\n",
            "Epoch --> 840\n",
            "Epoch --> 841\n",
            "Epoch --> 842\n",
            "Epoch --> 843\n",
            "Epoch --> 844\n",
            "Epoch --> 845\n",
            "Epoch --> 846\n",
            "Epoch --> 847\n",
            "Epoch --> 848\n",
            "Epoch --> 849\n",
            "Epoch --> 850\n",
            "Epoch --> 851\n",
            "Epoch --> 852\n",
            "Epoch --> 853\n",
            "Epoch --> 854\n",
            "Epoch --> 855\n",
            "Epoch --> 856\n",
            "Epoch --> 857\n",
            "Epoch --> 858\n",
            "Epoch --> 859\n",
            "Epoch --> 860\n",
            "Epoch --> 861\n",
            "Epoch --> 862\n",
            "Epoch --> 863\n",
            "Epoch --> 864\n",
            "Epoch --> 865\n",
            "Epoch --> 866\n",
            "Epoch --> 867\n",
            "Epoch --> 868\n",
            "Epoch --> 869\n",
            "Epoch --> 870\n",
            "Epoch --> 871\n",
            "Epoch --> 872\n",
            "Epoch --> 873\n",
            "Epoch --> 874\n",
            "Epoch --> 875\n",
            "Epoch --> 876\n",
            "Epoch --> 877\n",
            "Epoch --> 878\n",
            "Epoch --> 879\n",
            "Epoch --> 880\n",
            "Epoch --> 881\n",
            "Epoch --> 882\n",
            "Epoch --> 883\n",
            "Epoch --> 884\n",
            "Epoch --> 885\n",
            "Epoch --> 886\n",
            "Epoch --> 887\n",
            "Epoch --> 888\n",
            "Epoch --> 889\n",
            "Epoch --> 890\n",
            "Epoch --> 891\n",
            "Epoch --> 892\n",
            "Epoch --> 893\n",
            "Epoch --> 894\n",
            "Epoch --> 895\n",
            "Epoch --> 896\n",
            "Epoch --> 897\n",
            "Epoch --> 898\n",
            "Epoch --> 899\n",
            "Epoch --> 900\n",
            "Epoch --> 901\n",
            "Epoch --> 902\n",
            "Epoch --> 903\n",
            "Epoch --> 904\n",
            "Epoch --> 905\n",
            "Epoch --> 906\n",
            "Epoch --> 907\n",
            "Epoch --> 908\n",
            "Epoch --> 909\n",
            "Epoch --> 910\n",
            "Epoch --> 911\n",
            "Epoch --> 912\n",
            "Epoch --> 913\n",
            "Epoch --> 914\n",
            "Epoch --> 915\n",
            "Epoch --> 916\n",
            "Epoch --> 917\n",
            "Epoch --> 918\n",
            "Epoch --> 919\n",
            "Epoch --> 920\n",
            "Epoch --> 921\n",
            "Epoch --> 922\n",
            "Epoch --> 923\n",
            "Epoch --> 924\n",
            "Epoch --> 925\n",
            "Epoch --> 926\n",
            "Epoch --> 927\n",
            "Epoch --> 928\n",
            "Epoch --> 929\n",
            "Epoch --> 930\n",
            "Epoch --> 931\n",
            "Epoch --> 932\n",
            "Epoch --> 933\n",
            "Epoch --> 934\n",
            "Epoch --> 935\n",
            "Epoch --> 936\n",
            "Epoch --> 937\n",
            "Epoch --> 938\n",
            "Epoch --> 939\n",
            "Epoch --> 940\n",
            "Epoch --> 941\n",
            "Epoch --> 942\n",
            "Epoch --> 943\n",
            "Epoch --> 944\n",
            "Epoch --> 945\n",
            "Epoch --> 946\n",
            "Epoch --> 947\n",
            "Epoch --> 948\n",
            "Epoch --> 949\n",
            "Epoch --> 950\n",
            "Epoch --> 951\n",
            "Epoch --> 952\n",
            "Epoch --> 953\n",
            "Epoch --> 954\n",
            "Epoch --> 955\n",
            "Epoch --> 956\n",
            "Epoch --> 957\n",
            "Epoch --> 958\n",
            "Epoch --> 959\n",
            "Epoch --> 960\n",
            "Epoch --> 961\n",
            "Epoch --> 962\n",
            "Epoch --> 963\n",
            "Epoch --> 964\n",
            "Epoch --> 965\n",
            "Epoch --> 966\n",
            "Epoch --> 967\n",
            "Epoch --> 968\n",
            "Epoch --> 969\n",
            "Epoch --> 970\n",
            "Epoch --> 971\n",
            "Epoch --> 972\n",
            "Epoch --> 973\n",
            "Epoch --> 974\n",
            "Epoch --> 975\n",
            "Epoch --> 976\n",
            "Epoch --> 977\n",
            "Epoch --> 978\n",
            "Epoch --> 979\n",
            "Epoch --> 980\n",
            "Epoch --> 981\n",
            "Epoch --> 982\n",
            "Epoch --> 983\n",
            "Epoch --> 984\n",
            "Epoch --> 985\n",
            "Epoch --> 986\n",
            "Epoch --> 987\n",
            "Epoch --> 988\n",
            "Epoch --> 989\n",
            "Epoch --> 990\n",
            "Epoch --> 991\n",
            "Epoch --> 992\n",
            "Epoch --> 993\n",
            "Epoch --> 994\n",
            "Epoch --> 995\n",
            "Epoch --> 996\n",
            "Epoch --> 997\n",
            "Epoch --> 998\n",
            "Epoch --> 999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Draw weights"
      ],
      "metadata": {
        "id": "PMTVl1X60rE3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "f229c09c"
      },
      "outputs": [],
      "source": [
        "def draw_weights(synapses, Kx, Ky, ax=None):\n",
        "    # synapses: the weights\n",
        "    Kw = int(np.sqrt(synapses.shape[1]//3)) # i.e. 32\n",
        "    yy=0\n",
        "    HM=np.zeros((Kw*Ky, Kw*Kx, 3))\n",
        "    for y in range(Ky):\n",
        "        for x in range(Kx):\n",
        "            HM[y*Kw:(y+1)*Kw,x*Kw:(x+1)*Kw]=synapses[yy,:Kw*Kw*3].reshape(Kw, Kw, 3)\n",
        "            yy += 1\n",
        "\n",
        "    nc=np.amax(np.absolute(HM))\n",
        "    tmp = (HM-HM.min())\n",
        "    tmp /= tmp.max()\n",
        "    tmp *= 255\n",
        "    tmp = tmp.astype(np.uint8)\n",
        "    if ax is not None:\n",
        "        im = ax.imshow(tmp)\n",
        "        ax.axis('off')\n",
        "    else:\n",
        "        plt.clf()\n",
        "        im=plt.imshow(tmp.astype(np.uint8))\n",
        "        plt.axis('off')\n",
        "    fig.canvas.draw()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Multi layer linear bio classifier**\n",
        "\n",
        "The Multilayer Pipeline Bio Learner extends the single-layer model into multiple layers, forming a sequential processing pipeline. Each layer is trained independently using the same biologically inspired learning mechanism.\n",
        "The output of one layer becomes the input for the next, allowing for progressively more complex feature extraction. Finally, a fully connected layer is added on top, trained using backpropagation to specialize the model for a specific task (image classification) or prediction.\n",
        "\n",
        "![Multilayer Image](https://raw.githubusercontent.com/isottongloria/PMLS_Bio-Learning/main/multilayer.png)\n"
      ],
      "metadata": {
        "id": "ijDwDyff3MZC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# BioClassifier: the supervised classifier model with frozen unsupervised weights W\n",
        "class BioClassifier(nn.Module):\n",
        "    def __init__(self, W, out_features=10, n=4.5, beta=0.01):\n",
        "        super().__init__()\n",
        "\n",
        "        self.W = W.transpose(0, 1)\n",
        "        self.n = n\n",
        "        self.beta = beta\n",
        "\n",
        "        # Linear layer for supervised weights S (from hidden to output)\n",
        "        self.S = nn.Linear(W.size(0), out_features, bias=False)  # S learns during supervised phase\n",
        "\n",
        "    def forward(self, v):\n",
        "        # Move input v to the same device as self.W\n",
        "        v = v.to(self.W.device)\n",
        "\n",
        "        # Apply the frozen weights W\n",
        "        Wv = torch.matmul(v, self.W)  # (batch_sz, n_filters)\n",
        "        h = F.relu(Wv) ** self.n  # Apply nonlinearity with n exponent\n",
        "        Sh = self.S(h)  # Supervised layer S\n",
        "        c = torch.tanh(self.beta * Sh)  # Final output with tanh\n",
        "        return c\n",
        "\n",
        "\n",
        "def multi_layer_unsupervised_bio_learning(train_dataset, layers_config, n_epochs, batch_size,\n",
        "                                          learning_rate, precision, anti_hebbian_learning_strength,\n",
        "                                          lebesgue_norm, rank):\n",
        "    \"\"\"\n",
        "    Trains a multi-layer unsupervised model using a layer-wise approach.\n",
        "\n",
        "    Parameters:\n",
        "    - train_dataset: Input dataset (torch.Tensor), where each row is a training example.\n",
        "    - layers_config: A list with the number of hidden units for each layer.\n",
        "    - Other parameters are the same as for the unsupervised_bio_learning_layer function.\n",
        "\n",
        "    Returns:\n",
        "    - synapses_list: List of trained weights for each layer.\n",
        "    \"\"\"\n",
        "\n",
        "    synapses_list = []\n",
        "\n",
        "    # The number of input units for the current layer is the number of features from the previous layer\n",
        "    input_data = torch.stack([data[0].flatten() for data in train_dataset])\n",
        "    n_input = input_data.shape[1]\n",
        "    input_data = input_data.to(device)\n",
        "\n",
        "    for idx in range(len(layers_config)):\n",
        "\n",
        "        # number of output neurons/hidden layers\n",
        "        n_hidden = layers_config[idx]\n",
        "        print(f\"Training layer {idx + 1} with {n_hidden} hidden units.\")\n",
        "        print(f\"Training with {n_input} input units.\")\n",
        "\n",
        "        # compute the unsupervised weights\n",
        "        synapses = unsupervised_bio_learning(train_dataset,initial_synapses=None, n_hidden=n_hidden, n_epochs=n_epochs, batch_size=batch_size,\n",
        "                                              learning_rate=learning_rate, precision=precision, anti_hebbian_learning_strength=anti_hebbian_learning_strength,\n",
        "                                              lebesgue_norm=lebesgue_norm, rank=rank, skip=1)\n",
        "\n",
        "        synapses_list.append(synapses)\n",
        "\n",
        "        # Do a forward pass to infer the next input == 'train_dataset'\n",
        "        model = BioClassifier(synapses, out_features=n_hidden).to(device)\n",
        "        outputs = model(input_data)\n",
        "\n",
        "        n_input = outputs.shape[1]  # Pass the output to the next layer\n",
        "        print('next input', n_input)\n",
        "        restored_dataset = TensorDataset(outputs)   #.view(-1, 1, 28, 28)\n",
        "\n",
        "        # Update variables\n",
        "        train_dataset = restored_dataset\n",
        "        input_data = torch.stack([data[0].flatten().to(device) for data in train_dataset])\n",
        "\n",
        "    return synapses_list  # Return the list of all layers' weights\n"
      ],
      "metadata": {
        "id": "PzmnbcKw2UL3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fixed parameters in common to all unsupervised layers\n",
        "lebesgue_norm_values = 2\n",
        "rank_values = 2\n",
        "n_epochs_unsupervised = 50\n",
        "batch_size = 28\n",
        "learning_rate = 0.01\n",
        "precision = 0.1\n",
        "anti_hebbian_learning_strength = 0.4\n",
        "layers_config = [20, 10] # number of output neurons for each unsupervised layer\n",
        "\n",
        "# Output aesthetics parameters\n",
        "skip = 1\n",
        "Kx = 10\n",
        "Ky = 10\n",
        "\n",
        "synapses_list = multi_layer_unsupervised_bio_learning(train_dataset, layers_config, n_epochs=2, batch_size=100,\n",
        "                                          learning_rate=0.01, precision=0.1, anti_hebbian_learning_strength=0.3,\n",
        "                                          lebesgue_norm=2, rank=5)"
      ],
      "metadata": {
        "id": "6kHGPg7Q3O9s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00de9779-e028-4a72-ff37-98778cf02b83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training layer 1 with 20 hidden units.\n",
            "Training with 784 input units.\n",
            "Epoch --> 0\n",
            "Epoch --> 1\n",
            "next input 20\n",
            "Training layer 2 with 10 hidden units.\n",
            "Training with 20 input units.\n",
            "Epoch --> 0\n",
            "Epoch --> 1\n",
            "next input 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XgUdJXqoMsfb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}